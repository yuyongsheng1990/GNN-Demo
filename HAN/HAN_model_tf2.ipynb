{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a4e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\\ndependencies:\\n    tensorflow-2.11.0\\n    numpy-1.22.0\\n    networkx-2.6.3\\n    scipy-1.7.3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\n",
    "dependencies:\n",
    "    tensorflow-2.11.0\n",
    "    numpy-1.22.0\n",
    "    networkx-2.6.3\n",
    "    scipy-1.7.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7c1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a83d679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\GNN Algorithms\\\\HAN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87dee6",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f61b77",
   "metadata": {},
   "source": [
    "## data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca23177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e4615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Prepare adjacency matrix by expanding up to a given neighbourhood.\\n This will insert loops on every node.\\n Finally, the matrix is converted to bias vectors.\\n Expected shape: [graph, nodes, nodes]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1903d",
   "metadata": {},
   "source": [
    "### adj_to_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19132eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3,4)[0] + np.eye(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a260b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_bias(adj, sizes, nhood=1):  # 邻接矩阵adjacency matrix\n",
    "    nb_graphs = adj.shape[0]  # graph number\n",
    "    mt = np.empty(adj.shape)  # 根据给定的维度和数值类型，返回一个新的ndarray数组，其元素不进行初始化\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])  # 返回一个单位矩阵\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))  # 相乘\n",
    "        for i in range(sizes[g]):  # 这个应该可以简化，直接对整个数组元素做操作！！！\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)  # 科学计数法，2.5 x 10^(-27)表示为：2.5e-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de35a",
   "metadata": {},
   "source": [
    "###  loading_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce8a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "def loading_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5905152",
   "metadata": {},
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573400ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成掩码bool数组\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)  # 生成全是0的数组\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18145aa",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f12d9a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_str):  # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:  # 此人编码功底实在很烂\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))  # 序列化读出file对象\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = loading_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))  # 创建一个空的lil_matrix\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))  # 创建一个shape的全是0的数组\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()  # vstack按行拼接\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))  # 从列表字典中返回一个图，获取邻接矩阵\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])  # 生成掩码bool数组\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)  # 全为0的shape numpy数组\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170368d",
   "metadata": {},
   "source": [
    "### sparse_to_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3658f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''转换为稀疏矩阵tuple'''\n",
    "def to_tuple(mx):\n",
    "    if not sp.isspmatrix_coo(mx):\n",
    "        mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    values = mx.data\n",
    "    shape = mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14574b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e90dc9",
   "metadata": {},
   "source": [
    "### standardize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d379e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()  # 将稀疏矩阵转回numpy矩阵\n",
    "    mu = f[train_mask == True, :].mean(axis=0)  # 按行求平均\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]  # sigma>0 get bool array\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8000",
   "metadata": {},
   "source": [
    "### preprocess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1162540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # power数组元素求n次方，flatten是降到一维\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # isinf判断是否为无穷\n",
    "    r_mat_inv = sp.diags(r_inv)  # 从对角线构造一个稀疏矩阵。\n",
    "    features = r_mat_inv.dot(features)  # dot矩阵乘法\n",
    "    return features.todense(), sparse_to_tuple(features)  # todense()转换成密集矩阵numpy.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d23fc",
   "metadata": {},
   "source": [
    "### normalize_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2a875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix. 对称归一化邻接矩阵\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996acf6",
   "metadata": {},
   "source": [
    "### preprocess_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16124e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))  # 对角线为1的矩阵\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f759",
   "metadata": {},
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767be59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_slim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94785fad",
   "metadata": {},
   "source": [
    "### attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4824f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head(features, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False,\n",
    "              return_coef=False):\n",
    "    \"\"\"[summary]\n",
    "    multi-head attention计算\n",
    "    [description]\n",
    "    # forward；model = HeteGAT_multi\n",
    "    attns.append(layers.attn_head(features,            # list:3, tensor（1， 3025， 1870）\n",
    "                                bias_mat=bias_mat,     # list:2, tensor(1, 3025, 3025)\n",
    "                                out_sz=hid_units[0],   # hid_units:[8]，卷积核的个数\n",
    "                                activation=activation, # nonlinearity:tf.nn.elu\n",
    "                                in_drop=ffd_drop,      # tensor, ()\n",
    "                                coef_drop=attn_drop,   # tensor, ()\n",
    "                                residual=False))\n",
    "    Arguments:\n",
    "        features {[type]} -- shape=(batch_size, nb_nodes, fea_size))\n",
    "    \"\"\"\n",
    "    with tf.name_scope('my_attn'):  # 定义一个上下文管理器\n",
    "        if in_drop != 0.0:\n",
    "            features = tf.nn.dropout(features, 1.0 - in_drop)  # 以rate置0\n",
    "        features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
    "        \n",
    "        f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        \n",
    "        logits = f_1 + tf.transpose(f_2, [0, 2, 1])  # 转置         # (1, 3025, 3025)\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)  # (1, 3025, 3025)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            features_fts = tf.nn.dropout(features_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, features_fts)                       # (1, 3025, 8)\n",
    "        ret = tf_slim.bias_add(vals)  # 将bias向量加到value矩阵上      # (1. 3025， 8)\n",
    "\n",
    "        # residual connection 残差连接\n",
    "        if residual:\n",
    "            if features.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(features, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                features_fts = ret + features\n",
    "        if return_coef:\n",
    "            return activation(ret), coefs\n",
    "        else:\n",
    "            return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b21e1",
   "metadata": {},
   "source": [
    "### attn_head_const_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22b7c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head_const_1(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    \"\"\"[summary]\n",
    "    [description]\n",
    "    \"\"\"\n",
    "    adj_mat = 1.0 - bias_mat / -1e9\n",
    "    with tf.name_scope('my_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "        \n",
    "\n",
    "        logits = adj_mat \n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a977a72",
   "metadata": {},
   "source": [
    "### sp_attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6ac3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_attn_head(seq, out_sz, adj_mat, activation, nb_nodes, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('sp_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        f_1 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        logits = tf.sparse_add(adj_mat * f_1, adj_mat *\n",
    "                               tf.transpose(f_2, [0, 2, 1]))\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices,\n",
    "                                values=tf.nn.leaky_relu(logits.values),\n",
    "                                dense_shape=logits.dense_shape)\n",
    "        coefs = tf.sparse_softmax(lrelu)  # 将softmax应用于批量的N维SparseTensor\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                                    values=tf.nn.dropout(\n",
    "                                        coefs.values, 1.0 - coef_drop),\n",
    "                                    dense_shape=coefs.dense_shape)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n",
    "        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n",
    "        # The method will fail in all other cases!\n",
    "        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        seq_fts = tf.squeeze(seq_fts)  \n",
    "        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)  # SparseTensor稀疏矩阵乘法\n",
    "        vals = tf.expand_dims(vals, axis=0)  # 在0处扩展维度1\n",
    "        vals.set_shape([1, nb_nodes, out_sz])\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ae2a3",
   "metadata": {},
   "source": [
    "### SimpleAttLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff4a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_embed, att_val = layers.SimpleAttLayer(multi_embed, mp_att_size,\n",
    "#                                                      time_major=False,\n",
    "#                                                      return_alphas=True)\n",
    "def SimpleAttLayer(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    '''\n",
    "    inputs: tensor, (3025, 2, 64)\n",
    "    attention_size: 128\n",
    "    '''\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)  # 表示在shape第2个维度上拼接\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])  #\n",
    "\n",
    "    hidden_size = inputs.shape[2]  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.compat.v1.random_normal([hidden_size, attention_size], stddev=0.1))  # (64, 128)\n",
    "    b_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "    u_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)   # (3025, 2, 128)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape   tensor, (3025, 2)\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape   tensor, (3025, 2)\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)  # (3025, 2, 64) * (3025, 2, 1) = (3025, 2, 64) -> (3025, 2)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas  # attention输出、softmax概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72795e7",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15b06dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle  # 把训练好的模型存储起来\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from sklearn import manifold  # 一种非线性降维的手段\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7c941",
   "metadata": {},
   "source": [
    "## KNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ab62687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_KNN(x, y, k=5, split_list=[0.2, 0.4, 0.6, 0.8], time=10, show_train=True, shuffle=True):\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    for split in split_list:\n",
    "        ss = split\n",
    "        split = int(x.shape[0] * split)\n",
    "        micro_list = []\n",
    "        macro_list = []\n",
    "        if time:\n",
    "            for i in range(time):\n",
    "                if shuffle:\n",
    "                    permutation = np.random.permutation(x.shape[0])  # 生成一个随机打散的序列。\n",
    "                    x = x[permutation, :]\n",
    "                    y = y[permutation]\n",
    "                # x_true = np.array(x_true)\n",
    "                train_x = x[:split, :]\n",
    "                test_x = x[split:, :]\n",
    "\n",
    "                train_y = y[:split]\n",
    "                test_y = y[split:]\n",
    "\n",
    "                estimator = KNeighborsClassifier(n_neighbors=k)\n",
    "                estimator.fit(train_x, train_y)\n",
    "                y_pred = estimator.predict(test_x)\n",
    "                f1_macro = f1_score(test_y, y_pred, average='macro')\n",
    "                f1_micro = f1_score(test_y, y_pred, average='micro')\n",
    "                macro_list.append(f1_macro)\n",
    "                micro_list.append(f1_micro)\n",
    "            print('KNN({}avg, split:{}, k={}) f1_macro: {:.4f}, f1_micro: {:.4f}'.format(\n",
    "                time, ss, k, sum(macro_list) / len(macro_list), sum(micro_list) / len(micro_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cdedd",
   "metadata": {},
   "source": [
    "## kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5798d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_Kmeans(x, y, k=4, time=10, return_NMI=False):\n",
    "\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    ARI_list = []  # adjusted_rand_score(\n",
    "    NMI_list = []\n",
    "    if time:\n",
    "        # print('KMeans exps {}次 æ±~B平å~]~G '.format(time))\n",
    "        for i in range(time):\n",
    "            estimator.fit(x, y)\n",
    "            y_pred = estimator.predict(x)\n",
    "            score = normalized_mutual_info_score(y, y_pred)\n",
    "            NMI_list.append(score)\n",
    "            s2 = adjusted_rand_score(y, y_pred)\n",
    "            ARI_list.append(s2)\n",
    "        # print('NMI_list: {}'.format(NMI_list))\n",
    "        score = sum(NMI_list) / len(NMI_list)\n",
    "        s2 = sum(ARI_list) / len(ARI_list)\n",
    "        print('NMI (10 avg): {:.4f} , ARI (10avg): {:.4f}'.format(score, s2))\n",
    "\n",
    "    else:\n",
    "        estimator.fit(x, y)\n",
    "        y_pred = estimator.predict(x)\n",
    "        score = normalized_mutual_info_score(y, y_pred)\n",
    "        print(\"NMI on all label data: {:.5f}\".format(score))\n",
    "    if return_NMI:\n",
    "        return score, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69896b23",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d1fbd",
   "metadata": {},
   "source": [
    "## BaseGAttN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0fb822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable(np.random.random(size=(1,)))\n",
    "print(type([var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873432fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecedc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGAttN:\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(\n",
    "            tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(  # 计算多分类交叉熵\n",
    "            labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "    \n",
    "    # 更新梯度权重\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.compat.v1.trainable_variables()  # 查看可训练变量,list\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not    # add_n实现列表相加；l2_loss是l2范数值得一半\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "        # lossL2, tensor(mul_5.0), shape=()\n",
    "        # optimizer\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=lr)  # adam函数\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss + lossL2)  # 计算梯度，然后更新参数\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)  # 返回行最大值索引\n",
    "        return tf.confusion_matrix(labels, preds)  # 混淆矩阵\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # Adapted from tkipf/gcn #\n",
    "    ##########################\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(  # 返回交叉熵向量\n",
    "            logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)  # 改变tensor数据类型\n",
    "        mask /= tf.reduce_mean(mask)  # 通过均值求loss\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(  # sigmoid交叉熵\n",
    "            logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(loss, axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(   # 判断向量元素是否相等\n",
    "            tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "    \n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))  # 四舍五入函数\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "\n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)  # 非零元素个数\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9512b",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b5afdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f6d2f",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0cd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):  # 残差\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                          out_sz=hid_units[0], activation=activation,\n",
    "                                          in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        # multi-head attention\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[i], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                        out_sz=nb_classes, activation=lambda x: x,\n",
    "                                        in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3622",
   "metadata": {},
   "source": [
    "### HeteGAT_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64137920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT_multi(BaseGAttN):\n",
    "    '''\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor()\n",
    "                                                       hid_units=hid_units,   # hid_units:8\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    '''\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        for features, bias_mat in zip(ftr_in_list, bias_mat_list):\n",
    "            attns = []\n",
    "            jhy_embeds = []\n",
    "            for _ in range(n_heads[0]):   # [8,1]\n",
    "                # multi-head attention 计算\n",
    "                attns.append(attn_head(features, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop, residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))  # list:2. 其中每个元素tensor, (3025, 1, 64)\n",
    "\n",
    "        multi_embed = tf.concat(embed_list, axis=1)   # tensor, (3025, 2, 64)\n",
    "        # attention输出：tensor(3025, 64)、softmax概率\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, \n",
    "                                              mp_att_size,\n",
    "                                              time_major=False,\n",
    "                                              return_alphas=True)\n",
    "\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):  # 1\n",
    "            # 用于添加一个全连接层(input, output) -> (3025, 3)\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))  \n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]  # add_n是列表相加。tensor,(3025, 3)\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "\n",
    "        logits = tf.expand_dims(logits, axis=0)  # (1, 3025, 3)\n",
    "        # attention通过全连接层预测(1, 3025, 3)、attention final_embedding tensor(3025, 64)、attention 概率\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7de40d",
   "metadata": {},
   "source": [
    "### HeteGAT_no_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f795a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT_no_coef(BaseGAttN):\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, is_train, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        # coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "\n",
    "                attns.append(attn_head(ftr_in_list, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        # if return_coef:\n",
    "        #     return logits, final_embed, att_val, coef_list\n",
    "        # else:\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c6f62",
   "metadata": {},
   "source": [
    "### HeteGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4586445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128,\n",
    "                  return_coef=False):\n",
    "        embed_list = []\n",
    "        coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "                if return_coef:\n",
    "                    a1, a2 = attn_head(inputs, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                              return_coef=return_coef)\n",
    "                    attns.append(a1)\n",
    "                    head_coef_list.append(a2)\n",
    "                    # attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                               out_sz=hid_units[0], activation=activation,\n",
    "                    #                               in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                    #                               return_coef=return_coef)[0])\n",
    "                    #\n",
    "                    # head_coef_list.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                                        out_sz=hid_units[0], activation=activation,\n",
    "                    #                                        in_drop=ffd_drop, coef_drop=attn_drop,\n",
    "                    #                                        residual=False,\n",
    "                    #                                        return_coef=return_coef)[1])\n",
    "                else:\n",
    "                    attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            head_coef = tf.concat(head_coef_list, axis=0)\n",
    "            head_coef = tf.reduce_mean(head_coef, axis=0)\n",
    "            coef_list.append(head_coef)\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        if return_coef:\n",
    "            return logits, final_embed, att_val, coef_list\n",
    "        else:\n",
    "            return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adf56d",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3b4f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'  # 设置使用GPU1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c4e7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()  # 用来对session进行参数配置\n",
    "config.gpu_options.allow_growth = True  # 允许tf自动选择一个存在并且可用的设备来运行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74b23b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: D:\\PycharmProjects\\GNN Algorithms/data/han_data/acm_features.ckpt\n",
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.001\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000001F7B13444C0>\n",
      "model: <class '__main__.HeteGAT_multi'>\n"
     ]
    }
   ],
   "source": [
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file = os.path.abspath(os.path.dirname(os.getcwd())) +\\\n",
    "                            '/data/han_data/acm_features.ckpt'\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 200\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.001  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [8, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692361ad",
   "metadata": {},
   "source": [
    "## jhy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c8d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b08d6c",
   "metadata": {},
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21589aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af99c",
   "metadata": {},
   "source": [
    "### load_data_ACM3025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "930d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "acm_filepath = os.path.abspath(os.path.dirname(os.getcwd())) + '/data/han_data/ACM3025.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9207a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_acm3025(path=acm_filepath):\n",
    "    data = sio.loadmat(path)  # load .mat file\n",
    "    '''\n",
    "    全是ndarray\n",
    "    PTP: (3025, 3025)，全是1\n",
    "    PLP: (3025, 3025)，有0有1，有些向量时相同的\n",
    "    PAP: (3025, 3025)，对角线全是1，其他元素基本是0，很少是1.\n",
    "    feature: (3025, 1870)，由0、1组成。\n",
    "    label: (3025, 3),就3列，1061、965、999\n",
    "    train_idx: (1, 600)，0-2225随机抽取的索引\n",
    "    val_idx: (1, 300)，200-2325之间随机抽取的索引\n",
    "    test_idx: (1, 2125)，300-3024之间随机抽取的索引\n",
    "    '''\n",
    "    truelabels, truefeatures = data['label'], data['feature'].astype(float)\n",
    "    N = truefeatures.shape[0]\n",
    "    rownetworks = [data['PAP'] - np.eye(N), data['PLP'] - np.eye(N)]  # , data['PTP'] - np.eye(N)]\n",
    "    '''\n",
    "    rownetworks: list: 2。第1个元素，ndarray,(3025, 3025)；第2个元素，ndarray，(3025, 3025)\n",
    "    '''\n",
    "    y = truelabels    # shape为(3025, 3)\n",
    "    train_idx = data['train_idx']  # (1, 600)\n",
    "    val_idx = data['val_idx']      # (1, 300)\n",
    "    test_idx = data['test_idx']    # (1, 2125)\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])  # 3025长度的bool list，train_idx位置为True\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])      # 3025长度的boolean list\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    # 提取train、val、test的标签\n",
    "    # 所以，为什么不直接用train_test_split呢？\n",
    "    y_train = np.zeros(y.shape)  # shape为(3025, 3)的zero 列表\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y_train[train_mask, :] = y[train_mask, :]  # 取出train_idx为true的label，放入y_train，y_train其余位置为0\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures, truefeatures]   # truefeatures: (3025, 1870)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dd88a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:(3025, 3), y_val:(3025, 3), y_test:(3025, 3), train_idx:(1, 600), val_idx:(1, 300), test_idx:(1, 2125)\n"
     ]
    }
   ],
   "source": [
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_acm3025()\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56394450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3025, 1870)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "847f4938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0444ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3143462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57b9af",
   "metadata": {},
   "source": [
    "## run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fa80a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7f3c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "x = x[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e5d3d",
   "metadata": {},
   "source": [
    "### add_data_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8385aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truefeatures: (3025, 1870)\n",
    "nb_nodes = fea_list[0].shape[0]  # 3025\n",
    "ft_size = fea_list[0].shape[1]   # 1870\n",
    "nb_classes = y_train.shape[1]    # 3\n",
    "\n",
    "# adj = adj.todense()\n",
    "\n",
    "# features = features[np.newaxis]  # [1, nb_node, ft_size]\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]  # np.newaxis行增加一个新的维度\n",
    "'''\n",
    "fea_list:list:3。第一个元素，ndarray, (1, 3025, 1870)\n",
    "'''\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]  # adj_list: 2. 单个元素(1, 3025, 3025)\n",
    "y_train = y_train[np.newaxis]  # ndarray: (1, 3025, 3)\n",
    "y_val = y_val[np.newaxis]      # ndarray: (1, 3025, 3)\n",
    "y_test = y_test[np.newaxis]    # ndarray: (1, 3025, 3)\n",
    "train_mask = train_mask[np.newaxis]  # ndarray(1, 3025)\n",
    "val_mask = val_mask[np.newaxis]      # ndarray(1, 3025)\n",
    "test_mask = test_mask[np.newaxis]    # ndarray(1, 3025)\n",
    "\n",
    "biases_list = [adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af0066ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3025, 1870)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402472f",
   "metadata": {},
   "source": [
    "### build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6dec9169",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\3938853343.py:20: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\3938853343.py:22: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\3938853343.py:23: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\3264048851.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "Epoch: 0, att_val: [0.51276505 0.48723507]\n",
      "Training: loss = 1.10533, acc = 0.35833 | Val: loss = 1.09895, acc = 0.33333\n",
      "Epoch: 1, att_val: [0.5235428  0.47645727]\n",
      "Training: loss = 0.84997, acc = 0.54000 | Val: loss = 1.09880, acc = 0.33333\n",
      "Epoch: 2, att_val: [0.4789523  0.52104765]\n",
      "Training: loss = 0.63966, acc = 0.82833 | Val: loss = 1.09906, acc = 0.33333\n",
      "Epoch: 3, att_val: [0.85173255 0.14826684]\n",
      "Training: loss = 0.38729, acc = 0.93833 | Val: loss = 1.09954, acc = 0.33333\n",
      "Epoch: 4, att_val: [0.95619106 0.04380855]\n",
      "Training: loss = 0.29435, acc = 0.94667 | Val: loss = 1.10011, acc = 0.33333\n",
      "Epoch: 5, att_val: [0.97360665 0.02639408]\n",
      "Training: loss = 0.20888, acc = 0.96167 | Val: loss = 1.10075, acc = 0.33333\n",
      "Epoch: 6, att_val: [0.97826666 0.02173389]\n",
      "Training: loss = 0.16804, acc = 0.96167 | Val: loss = 1.10141, acc = 0.33333\n",
      "Epoch: 7, att_val: [0.97838014 0.02161961]\n",
      "Training: loss = 0.14202, acc = 0.96833 | Val: loss = 1.10205, acc = 0.33333\n",
      "Epoch: 8, att_val: [0.9767522  0.02324728]\n",
      "Training: loss = 0.12345, acc = 0.97000 | Val: loss = 1.10268, acc = 0.33333\n",
      "Epoch: 9, att_val: [0.97494334 0.02505705]\n",
      "Training: loss = 0.11708, acc = 0.96833 | Val: loss = 1.10329, acc = 0.33333\n",
      "Epoch: 10, att_val: [0.9670524  0.03294811]\n",
      "Training: loss = 0.09225, acc = 0.97667 | Val: loss = 1.10387, acc = 0.33333\n",
      "Epoch: 11, att_val: [0.9571256  0.04287463]\n",
      "Training: loss = 0.09221, acc = 0.97333 | Val: loss = 1.10442, acc = 0.33333\n",
      "Epoch: 12, att_val: [0.94779056 0.05220957]\n",
      "Training: loss = 0.06416, acc = 0.98333 | Val: loss = 1.10495, acc = 0.33333\n",
      "Epoch: 13, att_val: [0.9253284  0.07467142]\n",
      "Training: loss = 0.06794, acc = 0.98000 | Val: loss = 1.10536, acc = 0.33333\n",
      "Epoch: 14, att_val: [0.90527177 0.0947291 ]\n",
      "Training: loss = 0.06001, acc = 0.97667 | Val: loss = 1.10555, acc = 0.33333\n",
      "Epoch: 15, att_val: [0.91511244 0.08488774]\n",
      "Training: loss = 0.05021, acc = 0.98167 | Val: loss = 1.10557, acc = 0.33333\n",
      "Epoch: 16, att_val: [0.94824463 0.05175449]\n",
      "Training: loss = 0.04709, acc = 0.98500 | Val: loss = 1.10557, acc = 0.33333\n",
      "Epoch: 17, att_val: [0.9614354  0.03856542]\n",
      "Training: loss = 0.04835, acc = 0.98667 | Val: loss = 1.10561, acc = 0.33333\n",
      "Epoch: 18, att_val: [0.9683384  0.03166261]\n",
      "Training: loss = 0.03780, acc = 0.98667 | Val: loss = 1.10569, acc = 0.33333\n",
      "Epoch: 19, att_val: [0.9608565  0.03914325]\n",
      "Training: loss = 0.03279, acc = 0.99167 | Val: loss = 1.10574, acc = 0.33333\n",
      "Epoch: 20, att_val: [0.9603672  0.03963308]\n",
      "Training: loss = 0.02727, acc = 0.99333 | Val: loss = 1.10578, acc = 0.33333\n",
      "Epoch: 21, att_val: [0.94687474 0.05312503]\n",
      "Training: loss = 0.02428, acc = 0.99333 | Val: loss = 1.10581, acc = 0.33333\n",
      "Epoch: 22, att_val: [0.93741226 0.06258879]\n",
      "Training: loss = 0.03894, acc = 0.97833 | Val: loss = 1.10568, acc = 0.33333\n",
      "Epoch: 23, att_val: [0.9407669  0.05923282]\n",
      "Training: loss = 0.01829, acc = 0.99833 | Val: loss = 1.10553, acc = 0.33333\n",
      "Epoch: 24, att_val: [0.9420242  0.05797476]\n",
      "Training: loss = 0.03480, acc = 0.99167 | Val: loss = 1.10552, acc = 0.33333\n",
      "Epoch: 25, att_val: [0.9357204 0.0642786]\n",
      "Training: loss = 0.02788, acc = 0.99000 | Val: loss = 1.10562, acc = 0.33333\n",
      "Epoch: 26, att_val: [0.9120025  0.08799631]\n",
      "Training: loss = 0.01185, acc = 0.99667 | Val: loss = 1.10577, acc = 0.33333\n",
      "Epoch: 27, att_val: [0.8887375  0.11126154]\n",
      "Training: loss = 0.02089, acc = 0.99500 | Val: loss = 1.10588, acc = 0.33333\n",
      "Epoch: 28, att_val: [0.87582767 0.12417195]\n",
      "Training: loss = 0.03080, acc = 0.98833 | Val: loss = 1.10584, acc = 0.33333\n",
      "Epoch: 29, att_val: [0.90103894 0.09896135]\n",
      "Training: loss = 0.01015, acc = 0.99667 | Val: loss = 1.10580, acc = 0.33333\n",
      "Epoch: 30, att_val: [0.9155307  0.08446983]\n",
      "Training: loss = 0.01057, acc = 0.99833 | Val: loss = 1.10574, acc = 0.33333\n",
      "Epoch: 31, att_val: [0.93236125 0.0676391 ]\n",
      "Training: loss = 0.01606, acc = 0.99500 | Val: loss = 1.10570, acc = 0.33333\n",
      "Epoch: 32, att_val: [0.9329705  0.06702906]\n",
      "Training: loss = 0.01174, acc = 0.99833 | Val: loss = 1.10566, acc = 0.33333\n",
      "Epoch: 33, att_val: [0.93675363 0.06324558]\n",
      "Training: loss = 0.01194, acc = 0.99667 | Val: loss = 1.10560, acc = 0.33333\n",
      "Epoch: 34, att_val: [0.9430551  0.05694307]\n",
      "Training: loss = 0.01231, acc = 0.99833 | Val: loss = 1.10554, acc = 0.33333\n",
      "Epoch: 35, att_val: [0.9445632  0.05543837]\n",
      "Training: loss = 0.01682, acc = 0.99500 | Val: loss = 1.10558, acc = 0.33333\n",
      "Epoch: 36, att_val: [0.9389133  0.06108617]\n",
      "Training: loss = 0.01390, acc = 0.99833 | Val: loss = 1.10561, acc = 0.33333\n",
      "Epoch: 37, att_val: [0.92825556 0.0717429 ]\n",
      "Training: loss = 0.01073, acc = 0.99833 | Val: loss = 1.10567, acc = 0.33333\n",
      "Epoch: 38, att_val: [0.9216204  0.07838009]\n",
      "Training: loss = 0.01556, acc = 0.99500 | Val: loss = 1.10575, acc = 0.33333\n",
      "Epoch: 39, att_val: [0.9025125  0.09748641]\n",
      "Training: loss = 0.00798, acc = 0.99833 | Val: loss = 1.10576, acc = 0.33333\n",
      "Epoch: 40, att_val: [0.89336115 0.10663895]\n",
      "Training: loss = 0.01078, acc = 0.99667 | Val: loss = 1.10566, acc = 0.33333\n",
      "Epoch: 41, att_val: [0.89171505 0.10828509]\n",
      "Training: loss = 0.00970, acc = 0.99667 | Val: loss = 1.10552, acc = 0.33333\n",
      "Epoch: 42, att_val: [0.8872417  0.11275886]\n",
      "Training: loss = 0.00856, acc = 1.00000 | Val: loss = 1.10535, acc = 0.33333\n",
      "Epoch: 43, att_val: [0.884653   0.11534793]\n",
      "Training: loss = 0.00746, acc = 1.00000 | Val: loss = 1.10520, acc = 0.33333\n",
      "Epoch: 44, att_val: [0.8844702  0.11552998]\n",
      "Training: loss = 0.00844, acc = 1.00000 | Val: loss = 1.10510, acc = 0.33333\n",
      "Epoch: 45, att_val: [0.8816606  0.11833977]\n",
      "Training: loss = 0.01091, acc = 0.99833 | Val: loss = 1.10508, acc = 0.33333\n",
      "Epoch: 46, att_val: [0.8698085  0.13019054]\n",
      "Training: loss = 0.00710, acc = 1.00000 | Val: loss = 1.10505, acc = 0.33333\n",
      "Epoch: 47, att_val: [0.8509728  0.14902808]\n",
      "Training: loss = 0.00830, acc = 0.99667 | Val: loss = 1.10497, acc = 0.33333\n",
      "Epoch: 48, att_val: [0.8419345  0.15806645]\n",
      "Training: loss = 0.01022, acc = 0.99667 | Val: loss = 1.10484, acc = 0.33333\n",
      "Epoch: 49, att_val: [0.84467965 0.15532088]\n",
      "Training: loss = 0.01074, acc = 0.99667 | Val: loss = 1.10468, acc = 0.33333\n",
      "Epoch: 50, att_val: [0.8494034  0.15059642]\n",
      "Training: loss = 0.00548, acc = 1.00000 | Val: loss = 1.10454, acc = 0.33333\n",
      "Epoch: 51, att_val: [0.8526303 0.1473702]\n",
      "Training: loss = 0.00561, acc = 1.00000 | Val: loss = 1.10445, acc = 0.33333\n",
      "Epoch: 52, att_val: [0.83948225 0.16051774]\n",
      "Training: loss = 0.01239, acc = 0.99833 | Val: loss = 1.10429, acc = 0.33333\n",
      "Epoch: 53, att_val: [0.84861255 0.15138756]\n",
      "Training: loss = 0.00943, acc = 0.99833 | Val: loss = 1.10425, acc = 0.33333\n",
      "Epoch: 54, att_val: [0.83310896 0.16689071]\n",
      "Training: loss = 0.00622, acc = 1.00000 | Val: loss = 1.10421, acc = 0.33333\n",
      "Epoch: 55, att_val: [0.8276296  0.17237061]\n",
      "Training: loss = 0.00855, acc = 1.00000 | Val: loss = 1.10416, acc = 0.33333\n",
      "Epoch: 56, att_val: [0.81646067 0.18353942]\n",
      "Training: loss = 0.01476, acc = 0.99667 | Val: loss = 1.10412, acc = 0.33333\n",
      "Epoch: 57, att_val: [0.80621135 0.19378951]\n",
      "Training: loss = 0.01005, acc = 0.99833 | Val: loss = 1.10395, acc = 0.33333\n",
      "Epoch: 58, att_val: [0.81875736 0.18124257]\n",
      "Training: loss = 0.01315, acc = 0.99500 | Val: loss = 1.10374, acc = 0.33333\n",
      "Epoch: 59, att_val: [0.83827025 0.16172896]\n",
      "Training: loss = 0.00765, acc = 1.00000 | Val: loss = 1.10355, acc = 0.33333\n",
      "Epoch: 60, att_val: [0.8492781  0.15072194]\n",
      "Training: loss = 0.01289, acc = 0.99500 | Val: loss = 1.10333, acc = 0.33333\n",
      "Epoch: 61, att_val: [0.8582261 0.1417759]\n",
      "Training: loss = 0.00628, acc = 0.99833 | Val: loss = 1.10318, acc = 0.33333\n",
      "Epoch: 62, att_val: [0.86743414 0.13256553]\n",
      "Training: loss = 0.01309, acc = 0.99500 | Val: loss = 1.10318, acc = 0.33333\n",
      "Epoch: 63, att_val: [0.84332234 0.15667868]\n",
      "Training: loss = 0.00889, acc = 1.00000 | Val: loss = 1.10325, acc = 0.33333\n",
      "Epoch: 64, att_val: [0.80960685 0.19039334]\n",
      "Training: loss = 0.01146, acc = 0.99833 | Val: loss = 1.10321, acc = 0.33333\n",
      "Epoch: 65, att_val: [0.79144895 0.20855124]\n",
      "Training: loss = 0.00720, acc = 1.00000 | Val: loss = 1.10312, acc = 0.33333\n",
      "Epoch: 66, att_val: [0.79404396 0.20595725]\n",
      "Training: loss = 0.00955, acc = 0.99833 | Val: loss = 1.10296, acc = 0.33333\n",
      "Epoch: 67, att_val: [0.8096357  0.19036527]\n",
      "Training: loss = 0.00924, acc = 1.00000 | Val: loss = 1.10279, acc = 0.33333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, att_val: [0.82787997 0.17212015]\n",
      "Training: loss = 0.01313, acc = 0.99833 | Val: loss = 1.10268, acc = 0.33333\n",
      "Epoch: 69, att_val: [0.82574475 0.17425531]\n",
      "Training: loss = 0.01333, acc = 0.99833 | Val: loss = 1.10251, acc = 0.33333\n",
      "Epoch: 70, att_val: [0.8446    0.1553999]\n",
      "Training: loss = 0.01020, acc = 1.00000 | Val: loss = 1.10242, acc = 0.33333\n",
      "Epoch: 71, att_val: [0.8366861  0.16331442]\n",
      "Training: loss = 0.00672, acc = 1.00000 | Val: loss = 1.10238, acc = 0.33333\n",
      "Epoch: 72, att_val: [0.8161653  0.18383457]\n",
      "Training: loss = 0.00976, acc = 1.00000 | Val: loss = 1.10235, acc = 0.33333\n",
      "Epoch: 73, att_val: [0.79995805 0.20004283]\n",
      "Training: loss = 0.00796, acc = 1.00000 | Val: loss = 1.10223, acc = 0.33333\n",
      "Epoch: 74, att_val: [0.79127866 0.20872176]\n",
      "Training: loss = 0.01651, acc = 0.99500 | Val: loss = 1.10194, acc = 0.33333\n",
      "Epoch: 75, att_val: [0.8273051  0.17269638]\n",
      "Training: loss = 0.00572, acc = 1.00000 | Val: loss = 1.10169, acc = 0.33333\n",
      "Epoch: 76, att_val: [0.8481635  0.15183666]\n",
      "Training: loss = 0.00669, acc = 1.00000 | Val: loss = 1.10160, acc = 0.33333\n",
      "Epoch: 77, att_val: [0.84584874 0.15415145]\n",
      "Training: loss = 0.00977, acc = 0.99833 | Val: loss = 1.10163, acc = 0.33333\n",
      "Epoch: 78, att_val: [0.8120756  0.18792424]\n",
      "Training: loss = 0.01212, acc = 0.99833 | Val: loss = 1.10160, acc = 0.33333\n",
      "Epoch: 79, att_val: [0.79052913 0.20947164]\n",
      "Training: loss = 0.01155, acc = 0.99667 | Val: loss = 1.10142, acc = 0.33333\n",
      "Epoch: 80, att_val: [0.8065481  0.19345082]\n",
      "Training: loss = 0.00845, acc = 1.00000 | Val: loss = 1.10119, acc = 0.33333\n",
      "Epoch: 81, att_val: [0.8279598  0.17204048]\n",
      "Training: loss = 0.00783, acc = 1.00000 | Val: loss = 1.10101, acc = 0.33333\n",
      "Epoch: 82, att_val: [0.8439488  0.15605123]\n",
      "Training: loss = 0.00704, acc = 1.00000 | Val: loss = 1.10097, acc = 0.33333\n",
      "Epoch: 83, att_val: [0.83044785 0.16955112]\n",
      "Training: loss = 0.00906, acc = 0.99833 | Val: loss = 1.10105, acc = 0.33333\n",
      "Epoch: 84, att_val: [0.7878083  0.21219091]\n",
      "Training: loss = 0.00854, acc = 1.00000 | Val: loss = 1.10115, acc = 0.33333\n",
      "Epoch: 85, att_val: [0.7236834 0.2763162]\n",
      "Training: loss = 0.01246, acc = 0.99833 | Val: loss = 1.10099, acc = 0.33333\n",
      "Epoch: 86, att_val: [0.764162   0.23583892]\n",
      "Training: loss = 0.00894, acc = 0.99833 | Val: loss = 1.10080, acc = 0.33333\n",
      "Epoch: 87, att_val: [0.78724456 0.21275549]\n",
      "Training: loss = 0.01585, acc = 0.99667 | Val: loss = 1.10048, acc = 0.33333\n",
      "Epoch: 88, att_val: [0.85191447 0.148086  ]\n",
      "Training: loss = 0.01267, acc = 0.99833 | Val: loss = 1.10045, acc = 0.33333\n",
      "Epoch: 89, att_val: [0.84301233 0.15698688]\n",
      "Training: loss = 0.01153, acc = 0.99833 | Val: loss = 1.10056, acc = 0.33333\n",
      "Epoch: 90, att_val: [0.800367  0.1996326]\n",
      "Training: loss = 0.00571, acc = 0.99833 | Val: loss = 1.10066, acc = 0.33333\n",
      "Epoch: 91, att_val: [0.75241286 0.24758679]\n",
      "Training: loss = 0.01637, acc = 0.99500 | Val: loss = 1.10041, acc = 0.33333\n",
      "Epoch: 92, att_val: [0.82697386 0.17302641]\n",
      "Training: loss = 0.00906, acc = 0.99833 | Val: loss = 1.10019, acc = 0.33333\n",
      "Epoch: 93, att_val: [0.88149333 0.11850604]\n",
      "Training: loss = 0.01546, acc = 1.00000 | Val: loss = 1.10033, acc = 0.33333\n",
      "Epoch: 94, att_val: [0.8457569  0.15424329]\n",
      "Training: loss = 0.01422, acc = 0.99667 | Val: loss = 1.10067, acc = 0.33333\n",
      "Epoch: 95, att_val: [0.74124646 0.25875267]\n",
      "Training: loss = 0.01541, acc = 0.99500 | Val: loss = 1.10070, acc = 0.33333\n",
      "Epoch: 96, att_val: [0.72785646 0.27214348]\n",
      "Training: loss = 0.00923, acc = 0.99833 | Val: loss = 1.10051, acc = 0.33333\n",
      "Epoch: 97, att_val: [0.76727015 0.23272933]\n",
      "Training: loss = 0.00927, acc = 0.99833 | Val: loss = 1.10021, acc = 0.33333\n",
      "Epoch: 98, att_val: [0.83388406 0.16611637]\n",
      "Training: loss = 0.00917, acc = 0.99667 | Val: loss = 1.09998, acc = 0.33333\n",
      "Epoch: 99, att_val: [0.87464046 0.12535964]\n",
      "Training: loss = 0.01482, acc = 0.99667 | Val: loss = 1.09999, acc = 0.33333\n",
      "Epoch: 100, att_val: [0.8667831  0.13321702]\n",
      "Training: loss = 0.01962, acc = 0.99500 | Val: loss = 1.10020, acc = 0.33333\n",
      "Epoch: 101, att_val: [0.81070834 0.18929   ]\n",
      "Training: loss = 0.01671, acc = 0.99667 | Val: loss = 1.10035, acc = 0.33333\n",
      "Epoch: 102, att_val: [0.7643172  0.23568283]\n",
      "Training: loss = 0.01198, acc = 0.99833 | Val: loss = 1.10035, acc = 0.33333\n",
      "Epoch: 103, att_val: [0.7599974  0.24000321]\n",
      "Training: loss = 0.00950, acc = 0.99667 | Val: loss = 1.10020, acc = 0.33333\n",
      "Epoch: 104, att_val: [0.79950607 0.20049298]\n",
      "Training: loss = 0.00649, acc = 1.00000 | Val: loss = 1.10006, acc = 0.33333\n",
      "Epoch: 105, att_val: [0.82261425 0.1773871 ]\n",
      "Training: loss = 0.00710, acc = 0.99833 | Val: loss = 1.09994, acc = 0.33333\n",
      "Epoch: 106, att_val: [0.84590507 0.15409376]\n",
      "Training: loss = 0.00676, acc = 1.00000 | Val: loss = 1.09991, acc = 0.33333\n",
      "Epoch: 107, att_val: [0.84502155 0.15497921]\n",
      "Training: loss = 0.01315, acc = 0.99500 | Val: loss = 1.09999, acc = 0.33333\n",
      "Epoch: 108, att_val: [0.8147465  0.18525302]\n",
      "Training: loss = 0.01310, acc = 0.99667 | Val: loss = 1.10010, acc = 0.33333\n",
      "Epoch: 109, att_val: [0.7772391  0.22276103]\n",
      "Training: loss = 0.00863, acc = 1.00000 | Val: loss = 1.10014, acc = 0.33333\n",
      "Epoch: 110, att_val: [0.7466787  0.25332168]\n",
      "Training: loss = 0.01644, acc = 0.99667 | Val: loss = 1.09996, acc = 0.33333\n",
      "Epoch: 111, att_val: [0.7858037  0.21419571]\n",
      "Training: loss = 0.01125, acc = 0.99667 | Val: loss = 1.09973, acc = 0.33333\n",
      "Epoch: 112, att_val: [0.84064734 0.15935129]\n",
      "Training: loss = 0.00760, acc = 1.00000 | Val: loss = 1.09966, acc = 0.33333\n",
      "Epoch: 113, att_val: [0.8592035  0.14079665]\n",
      "Training: loss = 0.00877, acc = 1.00000 | Val: loss = 1.09970, acc = 0.33333\n",
      "Epoch: 114, att_val: [0.83354366 0.1664561 ]\n",
      "Training: loss = 0.00967, acc = 1.00000 | Val: loss = 1.09984, acc = 0.33333\n",
      "Epoch: 115, att_val: [0.78517056 0.21483047]\n",
      "Training: loss = 0.00663, acc = 1.00000 | Val: loss = 1.09992, acc = 0.33333\n",
      "Epoch: 116, att_val: [0.7375761  0.26242572]\n",
      "Training: loss = 0.01142, acc = 0.99667 | Val: loss = 1.09980, acc = 0.33333\n",
      "Epoch: 117, att_val: [0.77220905 0.22779049]\n",
      "Training: loss = 0.01655, acc = 0.99667 | Val: loss = 1.09954, acc = 0.33333\n",
      "Epoch: 118, att_val: [0.8531995  0.14680162]\n",
      "Training: loss = 0.00719, acc = 1.00000 | Val: loss = 1.09939, acc = 0.33333\n",
      "Epoch: 119, att_val: [0.88883644 0.11116371]\n",
      "Training: loss = 0.01991, acc = 0.99667 | Val: loss = 1.09944, acc = 0.33333\n",
      "Epoch: 120, att_val: [0.85096896 0.14903095]\n",
      "Training: loss = 0.00744, acc = 1.00000 | Val: loss = 1.09954, acc = 0.33333\n",
      "Epoch: 121, att_val: [0.79870564 0.20129405]\n",
      "Training: loss = 0.00705, acc = 0.99667 | Val: loss = 1.09965, acc = 0.33333\n",
      "Epoch: 122, att_val: [0.73633003 0.26366967]\n",
      "Training: loss = 0.01976, acc = 0.99167 | Val: loss = 1.09951, acc = 0.33333\n",
      "Epoch: 123, att_val: [0.78553283 0.21446681]\n",
      "Training: loss = 0.00628, acc = 1.00000 | Val: loss = 1.09940, acc = 0.33333\n",
      "Epoch: 124, att_val: [0.8307641  0.16923623]\n",
      "Training: loss = 0.00997, acc = 0.99667 | Val: loss = 1.09938, acc = 0.33333\n",
      "Epoch: 125, att_val: [0.8597259  0.14027381]\n",
      "Training: loss = 0.00904, acc = 0.99833 | Val: loss = 1.09944, acc = 0.33333\n",
      "Epoch: 126, att_val: [0.8423693  0.15763108]\n",
      "Training: loss = 0.01104, acc = 0.99667 | Val: loss = 1.09958, acc = 0.33333\n",
      "Epoch: 127, att_val: [0.80069846 0.19930212]\n",
      "Training: loss = 0.00617, acc = 1.00000 | Val: loss = 1.09972, acc = 0.33333\n",
      "Epoch: 128, att_val: [0.7665024  0.23349744]\n",
      "Training: loss = 0.00663, acc = 0.99833 | Val: loss = 1.09980, acc = 0.33333\n",
      "Epoch: 129, att_val: [0.73817784 0.26182133]\n",
      "Training: loss = 0.01508, acc = 0.99667 | Val: loss = 1.09967, acc = 0.33333\n",
      "Epoch: 130, att_val: [0.78241146 0.21758822]\n",
      "Training: loss = 0.00814, acc = 0.99833 | Val: loss = 1.09956, acc = 0.33333\n",
      "Epoch: 131, att_val: [0.8151414  0.18485758]\n",
      "Training: loss = 0.00638, acc = 1.00000 | Val: loss = 1.09947, acc = 0.33333\n",
      "Epoch: 132, att_val: [0.83258766 0.16741352]\n",
      "Training: loss = 0.00852, acc = 0.99833 | Val: loss = 1.09940, acc = 0.33333\n",
      "Epoch: 133, att_val: [0.840029   0.15997085]\n",
      "Training: loss = 0.01082, acc = 1.00000 | Val: loss = 1.09939, acc = 0.33333\n",
      "Epoch: 134, att_val: [0.80906045 0.19093853]\n",
      "Training: loss = 0.00742, acc = 1.00000 | Val: loss = 1.09940, acc = 0.33333\n",
      "Epoch: 135, att_val: [0.78922313 0.21077706]\n",
      "Training: loss = 0.01370, acc = 0.99500 | Val: loss = 1.09934, acc = 0.33333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, att_val: [0.7853226  0.21467757]\n",
      "Training: loss = 0.00702, acc = 0.99833 | Val: loss = 1.09932, acc = 0.33333\n",
      "Epoch: 137, att_val: [0.78509814 0.21490103]\n",
      "Training: loss = 0.00720, acc = 0.99833 | Val: loss = 1.09931, acc = 0.33333\n",
      "Epoch: 138, att_val: [0.78545535 0.21454495]\n",
      "Training: loss = 0.00698, acc = 0.99833 | Val: loss = 1.09928, acc = 0.33333\n",
      "Epoch: 139, att_val: [0.80925757 0.19074197]\n",
      "Training: loss = 0.00613, acc = 1.00000 | Val: loss = 1.09926, acc = 0.33333\n",
      "Epoch: 140, att_val: [0.8282262 0.1717758]\n",
      "Training: loss = 0.00805, acc = 0.99833 | Val: loss = 1.09926, acc = 0.33333\n",
      "Epoch: 141, att_val: [0.8263175  0.17368262]\n",
      "Training: loss = 0.00971, acc = 0.99833 | Val: loss = 1.09928, acc = 0.33333\n",
      "Epoch: 142, att_val: [0.78264534 0.21735413]\n",
      "Training: loss = 0.01053, acc = 0.99833 | Val: loss = 1.09931, acc = 0.33333\n",
      "Epoch: 143, att_val: [0.7369499  0.26304987]\n",
      "Training: loss = 0.00998, acc = 0.99667 | Val: loss = 1.09931, acc = 0.33333\n",
      "Epoch: 144, att_val: [0.74209887 0.2579002 ]\n",
      "Training: loss = 0.01168, acc = 0.99833 | Val: loss = 1.09925, acc = 0.33333\n",
      "Epoch: 145, att_val: [0.81520575 0.1847947 ]\n",
      "Training: loss = 0.00676, acc = 1.00000 | Val: loss = 1.09927, acc = 0.33333\n",
      "Epoch: 146, att_val: [0.85677654 0.14322335]\n",
      "Training: loss = 0.01614, acc = 0.99667 | Val: loss = 1.09923, acc = 0.33333\n",
      "Epoch: 147, att_val: [0.79623675 0.20376292]\n",
      "Training: loss = 0.00836, acc = 0.99833 | Val: loss = 1.09925, acc = 0.33333\n",
      "Epoch: 148, att_val: [0.7110299  0.28897032]\n",
      "Training: loss = 0.01016, acc = 0.99833 | Val: loss = 1.09928, acc = 0.33333\n",
      "Epoch: 149, att_val: [0.6837671 0.3162328]\n",
      "Training: loss = 0.02059, acc = 0.99500 | Val: loss = 1.09923, acc = 0.33333\n",
      "Epoch: 150, att_val: [0.81310505 0.18689588]\n",
      "Training: loss = 0.00689, acc = 0.99833 | Val: loss = 1.09927, acc = 0.33333\n",
      "Epoch: 151, att_val: [0.8892705  0.11072848]\n",
      "Training: loss = 0.03491, acc = 0.98833 | Val: loss = 1.09933, acc = 0.33333\n",
      "Epoch: 152, att_val: [0.83317924 0.16682066]\n",
      "Training: loss = 0.00948, acc = 0.99833 | Val: loss = 1.09950, acc = 0.33333\n",
      "Epoch: 153, att_val: [0.7469989  0.25300056]\n",
      "Training: loss = 0.00961, acc = 0.99667 | Val: loss = 1.09968, acc = 0.33333\n",
      "Epoch: 154, att_val: [0.7013513  0.29864904]\n",
      "Training: loss = 0.02001, acc = 0.99667 | Val: loss = 1.09962, acc = 0.33333\n",
      "Epoch: 155, att_val: [0.7567092  0.24329108]\n",
      "Training: loss = 0.00821, acc = 0.99833 | Val: loss = 1.09952, acc = 0.33333\n",
      "Epoch: 156, att_val: [0.8209777  0.17902221]\n",
      "Training: loss = 0.00878, acc = 0.99833 | Val: loss = 1.09948, acc = 0.33333\n",
      "Epoch: 157, att_val: [0.8512269  0.14877361]\n",
      "Training: loss = 0.00995, acc = 0.99833 | Val: loss = 1.09945, acc = 0.33333\n",
      "Epoch: 158, att_val: [0.868656   0.13134463]\n",
      "Training: loss = 0.01150, acc = 0.99833 | Val: loss = 1.09947, acc = 0.33333\n",
      "Epoch: 159, att_val: [0.8660992  0.13390097]\n",
      "Training: loss = 0.00916, acc = 0.99833 | Val: loss = 1.09950, acc = 0.33333\n",
      "Epoch: 160, att_val: [0.84754074 0.15245856]\n",
      "Training: loss = 0.00672, acc = 1.00000 | Val: loss = 1.09956, acc = 0.33333\n",
      "Epoch: 161, att_val: [0.8191905 0.1808096]\n",
      "Training: loss = 0.00920, acc = 0.99667 | Val: loss = 1.09957, acc = 0.33333\n",
      "Epoch: 162, att_val: [0.80786175 0.19213815]\n",
      "Training: loss = 0.00910, acc = 0.99833 | Val: loss = 1.09955, acc = 0.33333\n",
      "Epoch: 163, att_val: [0.8010304 0.1989689]\n",
      "Training: loss = 0.00801, acc = 0.99833 | Val: loss = 1.09953, acc = 0.33333\n",
      "Epoch: 164, att_val: [0.80805516 0.1919437 ]\n",
      "Training: loss = 0.01302, acc = 0.99833 | Val: loss = 1.09952, acc = 0.33333\n",
      "Epoch: 165, att_val: [0.8132609 0.1867385]\n",
      "Training: loss = 0.00493, acc = 1.00000 | Val: loss = 1.09952, acc = 0.33333\n",
      "Epoch: 166, att_val: [0.82682717 0.17317268]\n",
      "Training: loss = 0.01416, acc = 0.99667 | Val: loss = 1.09953, acc = 0.33333\n",
      "Epoch: 167, att_val: [0.82652354 0.17347552]\n",
      "Training: loss = 0.01260, acc = 0.99667 | Val: loss = 1.09959, acc = 0.33333\n",
      "Epoch: 168, att_val: [0.7976218 0.2023776]\n",
      "Training: loss = 0.00960, acc = 1.00000 | Val: loss = 1.09967, acc = 0.33333\n",
      "Epoch: 169, att_val: [0.78306025 0.21693982]\n",
      "Training: loss = 0.00531, acc = 1.00000 | Val: loss = 1.09974, acc = 0.33333\n",
      "Epoch: 170, att_val: [0.7678617  0.23213767]\n",
      "Training: loss = 0.00544, acc = 1.00000 | Val: loss = 1.09980, acc = 0.33333\n",
      "Epoch: 171, att_val: [0.76012194 0.2398779 ]\n",
      "Training: loss = 0.00932, acc = 0.99833 | Val: loss = 1.09979, acc = 0.33333\n",
      "Epoch: 172, att_val: [0.76713866 0.2328612 ]\n",
      "Training: loss = 0.01358, acc = 0.99667 | Val: loss = 1.09972, acc = 0.33333\n",
      "Epoch: 173, att_val: [0.8036124 0.1963861]\n",
      "Training: loss = 0.00694, acc = 1.00000 | Val: loss = 1.09964, acc = 0.33333\n",
      "Epoch: 174, att_val: [0.82649827 0.173503  ]\n",
      "Training: loss = 0.01091, acc = 0.99500 | Val: loss = 1.09962, acc = 0.33333\n",
      "Epoch: 175, att_val: [0.8308878  0.16911304]\n",
      "Training: loss = 0.00927, acc = 1.00000 | Val: loss = 1.09964, acc = 0.33333\n",
      "Epoch: 176, att_val: [0.7965231  0.20347634]\n",
      "Training: loss = 0.00489, acc = 1.00000 | Val: loss = 1.09970, acc = 0.33333\n",
      "Epoch: 177, att_val: [0.73890924 0.2610902 ]\n",
      "Training: loss = 0.00990, acc = 0.99667 | Val: loss = 1.09968, acc = 0.33333\n",
      "Epoch: 178, att_val: [0.7477198  0.25227955]\n",
      "Training: loss = 0.00522, acc = 1.00000 | Val: loss = 1.09963, acc = 0.33333\n",
      "Epoch: 179, att_val: [0.76494414 0.23505571]\n",
      "Training: loss = 0.00604, acc = 1.00000 | Val: loss = 1.09962, acc = 0.33333\n",
      "Epoch: 180, att_val: [0.78514034 0.21486016]\n",
      "Training: loss = 0.00566, acc = 0.99833 | Val: loss = 1.09965, acc = 0.33333\n",
      "Epoch: 181, att_val: [0.7818749  0.21812485]\n",
      "Training: loss = 0.00697, acc = 1.00000 | Val: loss = 1.09969, acc = 0.33333\n",
      "Epoch: 182, att_val: [0.78131    0.21868972]\n",
      "Training: loss = 0.00922, acc = 0.99667 | Val: loss = 1.09982, acc = 0.33333\n",
      "Epoch: 183, att_val: [0.7610859 0.2389133]\n",
      "Training: loss = 0.01130, acc = 0.99500 | Val: loss = 1.09990, acc = 0.33333\n",
      "Epoch: 184, att_val: [0.7833241  0.21667525]\n",
      "Training: loss = 0.00662, acc = 0.99833 | Val: loss = 1.10001, acc = 0.33333\n",
      "Epoch: 185, att_val: [0.8130626  0.18693802]\n",
      "Training: loss = 0.00597, acc = 1.00000 | Val: loss = 1.10012, acc = 0.33333\n",
      "Epoch: 186, att_val: [0.82199275 0.17800646]\n",
      "Training: loss = 0.01258, acc = 0.99667 | Val: loss = 1.10023, acc = 0.33333\n",
      "Epoch: 187, att_val: [0.79940355 0.20059611]\n",
      "Training: loss = 0.00625, acc = 0.99833 | Val: loss = 1.10033, acc = 0.33333\n",
      "Epoch: 188, att_val: [0.7528009  0.24719952]\n",
      "Training: loss = 0.01080, acc = 0.99833 | Val: loss = 1.10033, acc = 0.33333\n",
      "Epoch: 189, att_val: [0.7715518  0.22844993]\n",
      "Training: loss = 0.00670, acc = 1.00000 | Val: loss = 1.10029, acc = 0.33333\n",
      "Epoch: 190, att_val: [0.7991215  0.20087822]\n",
      "Training: loss = 0.00627, acc = 1.00000 | Val: loss = 1.10024, acc = 0.33333\n",
      "Epoch: 191, att_val: [0.8113898  0.18861005]\n",
      "Training: loss = 0.00497, acc = 1.00000 | Val: loss = 1.10019, acc = 0.33333\n",
      "Epoch: 192, att_val: [0.82043195 0.17956713]\n",
      "Training: loss = 0.01016, acc = 0.99833 | Val: loss = 1.10011, acc = 0.33333\n",
      "Epoch: 193, att_val: [0.78326195 0.2167387 ]\n",
      "Training: loss = 0.00549, acc = 1.00000 | Val: loss = 1.10007, acc = 0.33333\n",
      "Epoch: 194, att_val: [0.7558664  0.24413343]\n",
      "Training: loss = 0.00652, acc = 1.00000 | Val: loss = 1.10009, acc = 0.33333\n",
      "Epoch: 195, att_val: [0.7501892  0.24981114]\n",
      "Training: loss = 0.00763, acc = 1.00000 | Val: loss = 1.10011, acc = 0.33333\n",
      "Epoch: 196, att_val: [0.7887283  0.21127293]\n",
      "Training: loss = 0.00843, acc = 0.99667 | Val: loss = 1.10020, acc = 0.33333\n",
      "Epoch: 197, att_val: [0.8334279  0.16657165]\n",
      "Training: loss = 0.00987, acc = 0.99833 | Val: loss = 1.10032, acc = 0.33333\n",
      "Epoch: 198, att_val: [0.81808245 0.1819182 ]\n",
      "Training: loss = 0.00678, acc = 1.00000 | Val: loss = 1.10042, acc = 0.33333\n",
      "Epoch: 199, att_val: [0.7719796  0.22802113]\n",
      "Training: loss = 0.00791, acc = 1.00000 | Val: loss = 1.10051, acc = 0.33333\n",
      "INFO:tensorflow:Restoring parameters from D:\\PycharmProjects\\GNN Algorithms/data/han_data/acm_features.ckpt\n",
      "load model from : D:\\PycharmProjects\\GNN Algorithms/data/han_data/acm_features.ckpt\n",
      "Test loss: 1.0996148586273193 ; Test accuracy: 0.3289412558078766\n",
      "start knn, kmean.....\n",
      "xx: (2125, 64), yy: (2125, 3)\n",
      "KNN(10avg, split:0.2, k=5) f1_macro: 0.1741, f1_micro: 0.3486\n",
      "KNN(10avg, split:0.4, k=5) f1_macro: 0.1738, f1_micro: 0.3451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(10avg, split:0.6, k=5) f1_macro: 0.1719, f1_micro: 0.3358\n",
      "KNN(10avg, split:0.8, k=5) f1_macro: 0.1731, f1_micro: 0.3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI (10 avg): 0.0000 , ARI (10avg): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9316\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n"
     ]
    }
   ],
   "source": [
    "print('build graph...')\n",
    "with tf.Graph().as_default():  # 创建一个新的计算图\n",
    "    with tf.name_scope('input'):  # 创建一个上下文管理器\n",
    "        ftr_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,  # 占位符，提前分配必要的内存\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),  # batch_size:1, nb_nodes:3025, fea_size: 1870\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]  # fea_list,长度为3，内部单个元素，(1, 3025, 1870)\n",
    "        bias_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]  # 邻接矩阵转换成的biases_list: 2. 单个元素占位符tensor, (1, 3025, 3025)\n",
    "        lbl_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes, nb_classes),   # tensor, nb_classes: 3\n",
    "                                        name='lbl_in')   \n",
    "        msk_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes),  # tensor, (1, 3025)\n",
    "                                        name='msk_in')\n",
    "        attn_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='attn_drop')  # tensor, ()\n",
    "        ffd_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')  # # tensor, ()\n",
    "        is_train = tf.compat.v1.placeholder(dtype=tf.bool, shape=(), name='is_train')  # # tensor, ()\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor(1, 3025, 3025)\n",
    "                                                       hid_units=hid_units,   # hid_units:[8]\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])  # （3025， 3）\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])  # （3025， 3）\n",
    "    msk_resh = tf.reshape(msk_in, [-1])              # mask，（3025， ）\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)  # 占位符计算softmax cross_entropy based on (pred, y)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)           # 计算accuracy\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)  # lr = 0.005、l2_coef = 0.001\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver()  # 用于保存模型\n",
    "\n",
    "    init_op = tf.group(tf.compat.v1.global_variables_initializer(),  # 全局变量初始化；group组合多个operation\n",
    "                       tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:  # 创建session\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):  # 200\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                # feature,占位符内存已经分配完毕，fea_list是真实数据，输入进行训练模型\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict:3. 每个元素tensor, (1, 3025, 1870)\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                # bias\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict: 2. 每个元素tensor, (1, 3025, 3025)\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                # other params\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0.6,\n",
    "                       ffd_drop: 0.6}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)  # 字典update方法\n",
    "                fd.update(fd3)  # 获得字典形式的所有数据、参数\n",
    "                # training操作：更新权重；计算loss；计算accuracy；attention概率\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "            # =============   val       =================\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                # fd1 = {ftr_in: features[vl_step * batch_size:(vl_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_val[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       msk_in: val_mask[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "          \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                                                 feed_dict=fd)\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step,\n",
    "                   val_loss_avg / vl_step, val_acc_avg / vl_step))\n",
    "            \n",
    "            # =============   judging  =================\n",
    "            if val_acc_avg / vl_step >= vacc_mx or val_loss_avg / vl_step <= vlss_mn:\n",
    "                if val_acc_avg / vl_step >= vacc_mx and val_loss_avg / vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg / vl_step\n",
    "                    vlss_early_model = val_loss_avg / vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg / vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg / vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn,\n",
    "                          ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ',\n",
    "                          vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        \n",
    "        # loading model params\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "        \n",
    "        # ============= testing =================\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "\n",
    "        print('start knn, kmean.....')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "  \n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        # xx = xx / LA.norm(xx, axis=1)\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "\n",
    "        my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f046e4",
   "metadata": {},
   "source": [
    "### debuge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac1313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcc166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "503.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
