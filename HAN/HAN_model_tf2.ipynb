{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41a4e52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\\ndependencies:\\n    tensorflow-2.11.0\\n    numpy-1.22.0\\n    networkx-2.6.3\\n    scipy-1.7.3\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\n",
    "dependencies:\n",
    "    tensorflow-2.11.0\n",
    "    numpy-1.22.0\n",
    "    networkx-2.6.3\n",
    "    scipy-1.7.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7c1403",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a83d679",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\yysgz\\\\OneDrive - Macquarie University\\\\Desktop\\\\GNN models\\\\HAN'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87dee6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f61b77",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca23177",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e4615c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Prepare adjacency matrix by expanding up to a given neighbourhood.\\n This will insert loops on every node.\\n Finally, the matrix is converted to bias vectors.\\n Expected shape: [graph, nodes, nodes]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1903d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### adj_to_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19132eb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3,4)[0] + np.eye(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a260b94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def adj_to_bias(adj, sizes, nhood=1):  # 邻接矩阵adjacency matrix\n",
    "    nb_graphs = adj.shape[0]  # graph number\n",
    "    mt = np.empty(adj.shape)  # 根据给定的维度和数值类型，返回一个新的ndarray数组，其元素不进行初始化\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])  # 返回一个单位矩阵\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))  # 相乘\n",
    "        for i in range(sizes[g]):  # 这个应该可以简化，直接对整个数组元素做操作！！！\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)  # 科学计数法，2.5 x 10^(-27)表示为：2.5e-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de35a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  loading_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ce8a118",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load file\n",
    "def loading_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5905152",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573400ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 生成掩码bool数组\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)  # 生成全是0的数组\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18145aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f12d9a",
   "metadata": {
    "code_folding": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_str):  # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:  # 此人编码功底实在很烂\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))  # 序列化读出file对象\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = loading_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))  # 创建一个空的lil_matrix\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))  # 创建一个shape的全是0的数组\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()  # vstack按行拼接\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))  # 从列表字典中返回一个图，获取邻接矩阵\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])  # 生成掩码bool数组\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)  # 全为0的shape numpy数组\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170368d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### sparse_to_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3658f9b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''转换为稀疏矩阵tuple'''\n",
    "def to_tuple(mx):\n",
    "    if not sp.isspmatrix_coo(mx):\n",
    "        mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    values = mx.data\n",
    "    shape = mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a14574b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e90dc9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### standardize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d379e49d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()  # 将稀疏矩阵转回numpy矩阵\n",
    "    mu = f[train_mask == True, :].mean(axis=0)  # 按行求平均\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]  # sigma>0 get bool array\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8000",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### preprocess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1162540",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # power数组元素求n次方，flatten是降到一维\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # isinf判断是否为无穷\n",
    "    r_mat_inv = sp.diags(r_inv)  # 从对角线构造一个稀疏矩阵。\n",
    "    features = r_mat_inv.dot(features)  # dot矩阵乘法\n",
    "    return features.todense(), sparse_to_tuple(features)  # todense()转换成密集矩阵numpy.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d23fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### normalize_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc2a875a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix. 对称归一化邻接矩阵\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996acf6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### preprocess_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16124e7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))  # 对角线为1的矩阵\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f759",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "767be59b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_slim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94785fad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4824f0f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def attn_head(features, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False,\n",
    "              return_coef=False):\n",
    "    \"\"\"[summary]\n",
    "    multi-head attention计算\n",
    "    [description]\n",
    "    # forward；model = HeteGAT_multi\n",
    "    attns.append(layers.attn_head(features,            # list:3, tensor（1， 3025， 1870）\n",
    "                                bias_mat=bias_mat,     # list:2, tensor(1, 3025, 3025)\n",
    "                                out_sz=hid_units[0],   # hid_units:[8]，卷积核的个数\n",
    "                                activation=activation, # nonlinearity:tf.nn.elu\n",
    "                                in_drop=ffd_drop,      # tensor, ()\n",
    "                                coef_drop=attn_drop,   # tensor, ()\n",
    "                                residual=False))\n",
    "    Arguments:\n",
    "        features {[type]} -- shape=(batch_size, nb_nodes, fea_size))\n",
    "    \"\"\"\n",
    "    with tf.name_scope('my_attn'):  # 定义一个上下文管理器\n",
    "        if in_drop != 0.0:\n",
    "            features = tf.nn.dropout(features, 1.0 - in_drop)  # 以rate置0\n",
    "        features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
    "        \n",
    "        f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        \n",
    "        logits = f_1 + tf.transpose(f_2, [0, 2, 1])  # 转置         # (1, 3025, 3025)\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)  # (1, 3025, 3025)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            features_fts = tf.nn.dropout(features_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, features_fts)                       # (1, 3025, 8)\n",
    "        ret = tf_slim.bias_add(vals)  # 将bias向量加到value矩阵上      # (1. 3025， 8)\n",
    "\n",
    "        # residual connection 残差连接\n",
    "        if residual:\n",
    "            if features.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(features, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                features_fts = ret + features\n",
    "        if return_coef:\n",
    "            return activation(ret), coefs\n",
    "        else:\n",
    "            return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b21e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### attn_head_const_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22b7c96c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def attn_head_const_1(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    \"\"\"[summary]\n",
    "    [description]\n",
    "    \"\"\"\n",
    "    adj_mat = 1.0 - bias_mat / -1e9\n",
    "    with tf.name_scope('my_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "        \n",
    "\n",
    "        logits = adj_mat \n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a977a72",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### sp_attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ac3fe1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sp_attn_head(seq, out_sz, adj_mat, activation, nb_nodes, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('sp_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        f_1 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        logits = tf.sparse_add(adj_mat * f_1, adj_mat *\n",
    "                               tf.transpose(f_2, [0, 2, 1]))\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices,\n",
    "                                values=tf.nn.leaky_relu(logits.values),\n",
    "                                dense_shape=logits.dense_shape)\n",
    "        coefs = tf.sparse_softmax(lrelu)  # 将softmax应用于批量的N维SparseTensor\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                                    values=tf.nn.dropout(\n",
    "                                        coefs.values, 1.0 - coef_drop),\n",
    "                                    dense_shape=coefs.dense_shape)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n",
    "        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n",
    "        # The method will fail in all other cases!\n",
    "        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        seq_fts = tf.squeeze(seq_fts)  \n",
    "        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)  # SparseTensor稀疏矩阵乘法\n",
    "        vals = tf.expand_dims(vals, axis=0)  # 在0处扩展维度1\n",
    "        vals.set_shape([1, nb_nodes, out_sz])\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ae2a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SimpleAttLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ff4a53a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# final_embed, att_val = layers.SimpleAttLayer(multi_embed, mp_att_size,\n",
    "#                                                      time_major=False,\n",
    "#                                                      return_alphas=True)\n",
    "def SimpleAttLayer(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    '''\n",
    "    inputs: tensor, (3025, 2, 64)\n",
    "    attention_size: 128\n",
    "    '''\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)  # 表示在shape第2个维度上拼接\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])  #\n",
    "\n",
    "    hidden_size = inputs.shape[2]  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.compat.v1.random_normal([hidden_size, attention_size], stddev=0.1))  # (64, 128)\n",
    "    b_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "    u_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)   # (3025, 2, 128)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape   tensor, (3025, 2)\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape   tensor, (3025, 2)\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)  # (3025, 2, 64) * (3025, 2, 1) = (3025, 2, 64) -> (3025, 2)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas  # attention输出、softmax概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72795e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15b06dc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle  # 把训练好的模型存储起来\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from sklearn import manifold  # 一种非线性降维的手段\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7c941",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## KNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ab62687",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_KNN(x, y, k=5, split_list=[0.2, 0.4, 0.6, 0.8], time=10, show_train=True, shuffle=True):\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    for split in split_list:\n",
    "        ss = split\n",
    "        split = int(x.shape[0] * split)\n",
    "        micro_list = []\n",
    "        macro_list = []\n",
    "        if time:\n",
    "            for i in range(time):\n",
    "                if shuffle:\n",
    "                    permutation = np.random.permutation(x.shape[0])  # 生成一个随机打散的序列。\n",
    "                    x = x[permutation, :]\n",
    "                    y = y[permutation]\n",
    "                # x_true = np.array(x_true)\n",
    "                train_x = x[:split, :]\n",
    "                test_x = x[split:, :]\n",
    "\n",
    "                train_y = y[:split]\n",
    "                test_y = y[split:]\n",
    "\n",
    "                estimator = KNeighborsClassifier(n_neighbors=k)\n",
    "                estimator.fit(train_x, train_y)\n",
    "                y_pred = estimator.predict(test_x)\n",
    "                f1_macro = f1_score(test_y, y_pred, average='macro')\n",
    "                f1_micro = f1_score(test_y, y_pred, average='micro')\n",
    "                macro_list.append(f1_macro)\n",
    "                micro_list.append(f1_micro)\n",
    "            print('KNN({}avg, split:{}, k={}) f1_macro: {:.4f}, f1_micro: {:.4f}'.format(\n",
    "                time, ss, k, sum(macro_list) / len(macro_list), sum(micro_list) / len(micro_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cdedd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5798d0fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_Kmeans(x, y, k=4, time=10, return_NMI=False):\n",
    "\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    ARI_list = []  # adjusted_rand_score(\n",
    "    NMI_list = []\n",
    "    if time:\n",
    "        # print('KMeans exps {}次 æ±~B平å~]~G '.format(time))\n",
    "        for i in range(time):\n",
    "            estimator.fit(x, y)\n",
    "            y_pred = estimator.predict(x)\n",
    "            score = normalized_mutual_info_score(y, y_pred)\n",
    "            NMI_list.append(score)\n",
    "            s2 = adjusted_rand_score(y, y_pred)\n",
    "            ARI_list.append(s2)\n",
    "        # print('NMI_list: {}'.format(NMI_list))\n",
    "        score = sum(NMI_list) / len(NMI_list)\n",
    "        s2 = sum(ARI_list) / len(ARI_list)\n",
    "        print('NMI (10 avg): {:.4f} , ARI (10avg): {:.4f}'.format(score, s2))\n",
    "\n",
    "    else:\n",
    "        estimator.fit(x, y)\n",
    "        y_pred = estimator.predict(x)\n",
    "        score = normalized_mutual_info_score(y, y_pred)\n",
    "        print(\"NMI on all label data: {:.5f}\".format(score))\n",
    "    if return_NMI:\n",
    "        return score, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69896b23",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d1fbd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BaseGAttN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0fb822f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable(np.random.random(size=(1,)))\n",
    "print(type([var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "873432fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecedc5be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseGAttN:\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(\n",
    "            tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(  # 计算多分类交叉熵\n",
    "            labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "    \n",
    "    # 更新梯度权重\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.compat.v1.trainable_variables()  # 查看可训练变量,list\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not    # add_n实现列表相加；l2_loss是l2范数值得一半\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "        # lossL2, tensor(mul_5.0), shape=()\n",
    "        # optimizer\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=lr)  # adam函数\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss + lossL2)  # 计算梯度，然后更新参数\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)  # 返回行最大值索引\n",
    "        return tf.confusion_matrix(labels, preds)  # 混淆矩阵\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # Adapted from tkipf/gcn #\n",
    "    ##########################\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(  # 返回交叉熵向量\n",
    "            logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)  # 改变tensor数据类型\n",
    "        mask /= tf.reduce_mean(mask)  # 通过均值求loss\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(  # sigmoid交叉熵\n",
    "            logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(loss, axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(   # 判断向量元素是否相等\n",
    "            tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "    \n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))  # 四舍五入函数\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "\n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)  # 非零元素个数\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9512b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b5afdb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f6d2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0cd1a97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):  # 残差\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                          out_sz=hid_units[0], activation=activation,\n",
    "                                          in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        # multi-head attention\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[i], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                        out_sz=nb_classes, activation=lambda x: x,\n",
    "                                        in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3622",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### HeteGAT_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64137920",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HeteGAT_multi(BaseGAttN):\n",
    "    '''\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor()\n",
    "                                                       hid_units=hid_units,   # hid_units:8\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    '''\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        for features, bias_mat in zip(ftr_in_list, bias_mat_list):\n",
    "            attns = []\n",
    "            jhy_embeds = []\n",
    "            for _ in range(n_heads[0]):   # [8,1]\n",
    "                # multi-head attention 计算\n",
    "                attns.append(attn_head(features, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop, residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))  # list:2. 其中每个元素tensor, (3025, 1, 64)\n",
    "\n",
    "        multi_embed = tf.concat(embed_list, axis=1)   # tensor, (3025, 2, 64)\n",
    "        # attention输出：tensor(3025, 64)、softmax概率\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, \n",
    "                                              mp_att_size,\n",
    "                                              time_major=False,\n",
    "                                              return_alphas=True)\n",
    "\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):  # 1\n",
    "            # 用于添加一个全连接层(input, output) -> (3025, 3)\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))  \n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]  # add_n是列表相加。tensor,(3025, 3)\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "\n",
    "        logits = tf.expand_dims(logits, axis=0)  # (1, 3025, 3)\n",
    "        # attention通过全连接层预测(1, 3025, 3)、attention final_embedding tensor(3025, 64)、attention 概率\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7de40d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### HeteGAT_no_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f795a54c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HeteGAT_no_coef(BaseGAttN):\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, is_train, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        # coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "\n",
    "                attns.append(attn_head(ftr_in_list, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        # if return_coef:\n",
    "        #     return logits, final_embed, att_val, coef_list\n",
    "        # else:\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c6f62",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### HeteGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4586445e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HeteGAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128,\n",
    "                  return_coef=False):\n",
    "        embed_list = []\n",
    "        coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "                if return_coef:\n",
    "                    a1, a2 = attn_head(inputs, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                              return_coef=return_coef)\n",
    "                    attns.append(a1)\n",
    "                    head_coef_list.append(a2)\n",
    "                    # attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                               out_sz=hid_units[0], activation=activation,\n",
    "                    #                               in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                    #                               return_coef=return_coef)[0])\n",
    "                    #\n",
    "                    # head_coef_list.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                                        out_sz=hid_units[0], activation=activation,\n",
    "                    #                                        in_drop=ffd_drop, coef_drop=attn_drop,\n",
    "                    #                                        residual=False,\n",
    "                    #                                        return_coef=return_coef)[1])\n",
    "                else:\n",
    "                    attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            head_coef = tf.concat(head_coef_list, axis=0)\n",
    "            head_coef = tf.reduce_mean(head_coef, axis=0)\n",
    "            coef_list.append(head_coef)\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        if return_coef:\n",
    "            return logits, final_embed, att_val, coef_list\n",
    "        else:\n",
    "            return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adf56d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b4f3fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'  # 设置使用GPU1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c4e7990",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()  # 用来对session进行参数配置\n",
    "config.gpu_options.allow_growth = True  # 允许tf自动选择一个存在并且可用的设备来运行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74b23b16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: C:\\Users\\yysgz\\OneDrive - Macquarie University\\Desktop\\GNN models/data/han_data/acm_features.ckpt\n",
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.001\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000001D5FFDD3EE0>\n",
      "model: <class '__main__.HeteGAT_multi'>\n"
     ]
    }
   ],
   "source": [
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file = os.path.abspath(os.path.dirname(os.getcwd())) +\\\n",
    "                            '/data/han_data/acm_features.ckpt'\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 200\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.001  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [8, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692361ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## jhy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c8d4888",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b08d6c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21589aa7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af99c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### load_data_ACM3025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "930d1608",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acm_filepath = os.path.abspath(os.path.dirname(os.getcwd())) + '/HAN/ACM3025.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9207a6f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_acm3025(path=acm_filepath):\n",
    "    data = sio.loadmat(path)  # load .mat file\n",
    "    '''\n",
    "    全是ndarray\n",
    "    PTP: (3025, 3025)，全是1\n",
    "    PLP: (3025, 3025)，有0有1，有些向量时相同的\n",
    "    PAP: (3025, 3025)，对角线全是1，其他元素基本是0，很少是1.\n",
    "    feature: (3025, 1870)，由0、1组成。\n",
    "    label: (3025, 3),就3列，1061、965、999\n",
    "    train_idx: (1, 600)，0-2225随机抽取的索引\n",
    "    val_idx: (1, 300)，200-2325之间随机抽取的索引\n",
    "    test_idx: (1, 2125)，300-3024之间随机抽取的索引\n",
    "    '''\n",
    "    truelabels, truefeatures = data['label'], data['feature'].astype(float)\n",
    "    N = truefeatures.shape[0]\n",
    "    rownetworks = [data['PAP'] - np.eye(N), data['PLP'] - np.eye(N)]  # , data['PTP'] - np.eye(N)]\n",
    "    '''\n",
    "    rownetworks: list: 2。第1个元素，ndarray,(3025, 3025)；第2个元素，ndarray，(3025, 3025)\n",
    "    '''\n",
    "    y = truelabels    # shape为(3025, 3)\n",
    "    train_idx = data['train_idx']  # (1, 600)\n",
    "    val_idx = data['val_idx']      # (1, 300)\n",
    "    test_idx = data['test_idx']    # (1, 2125)\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])  # 3025长度的bool list，train_idx位置为True\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])      # 3025长度的boolean list\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    # 提取train、val、test的标签\n",
    "    # 所以，为什么不直接用train_test_split呢？\n",
    "    y_train = np.zeros(y.shape)  # shape为(3025, 3)的zero 列表\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y_train[train_mask, :] = y[train_mask, :]  # 取出train_idx为true的label，放入y_train，y_train其余位置为0\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures, truefeatures]   # truefeatures: (3025, 1870)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dd88a9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:(3025, 3), y_val:(3025, 3), y_test:(3025, 3), train_idx:(1, 600), val_idx:(1, 300), test_idx:(1, 2125)\n"
     ]
    }
   ],
   "source": [
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_acm3025(acm_filepath)\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56394450",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3025, 1870)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "847f4938",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0444ae6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3143462f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57b9af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fa80a37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7f3c359",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "x = x[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e5d3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### add_data_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8385aef5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# truefeatures: (3025, 1870)\n",
    "nb_nodes = fea_list[0].shape[0]  # 3025\n",
    "ft_size = fea_list[0].shape[1]   # 1870\n",
    "nb_classes = y_train.shape[1]    # 3\n",
    "\n",
    "# adj = adj.todense()\n",
    "\n",
    "# features = features[np.newaxis]  # [1, nb_node, ft_size]\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]  # np.newaxis行增加一个新的维度\n",
    "'''\n",
    "fea_list:list:3。第一个元素，ndarray, (1, 3025, 1870)\n",
    "'''\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]  # adj_list: 2. 单个元素(1, 3025, 3025)\n",
    "y_train = y_train[np.newaxis]  # ndarray: (1, 3025, 3)\n",
    "y_val = y_val[np.newaxis]      # ndarray: (1, 3025, 3)\n",
    "y_test = y_test[np.newaxis]    # ndarray: (1, 3025, 3)\n",
    "train_mask = train_mask[np.newaxis]  # ndarray(1, 3025)\n",
    "val_mask = val_mask[np.newaxis]      # ndarray(1, 3025)\n",
    "test_mask = test_mask[np.newaxis]    # ndarray(1, 3025)\n",
    "\n",
    "biases_list = [adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af0066ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3025, 1870)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402472f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dec9169",
   "metadata": {
    "code_folding": [],
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_3240\\234240618.py:20: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_3240\\234240618.py:22: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_3240\\234240618.py:23: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_3240\\3264048851.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "Epoch: 0, att_val: [0.5526884  0.44731188]\n",
      "Training: loss = 1.11246, acc = 0.37833 | Val: loss = 1.09879, acc = 0.33333\n",
      "Epoch: 1, att_val: [0.28376234 0.71623755]\n",
      "Training: loss = 0.80428, acc = 0.79500 | Val: loss = 1.09878, acc = 0.33333\n",
      "Epoch: 2, att_val: [0.41670874 0.583292  ]\n",
      "Training: loss = 0.62982, acc = 0.83667 | Val: loss = 1.09901, acc = 0.33333\n",
      "Epoch: 3, att_val: [0.77200806 0.2279935 ]\n",
      "Training: loss = 0.42066, acc = 0.91000 | Val: loss = 1.09946, acc = 0.33333\n",
      "Epoch: 4, att_val: [0.82201624 0.17798382]\n",
      "Training: loss = 0.29309, acc = 0.95167 | Val: loss = 1.10004, acc = 0.33333\n",
      "Epoch: 5, att_val: [0.8103077  0.18969163]\n",
      "Training: loss = 0.21261, acc = 0.95333 | Val: loss = 1.10045, acc = 0.33333\n",
      "Epoch: 6, att_val: [0.8428286  0.15717141]\n",
      "Training: loss = 0.16974, acc = 0.95833 | Val: loss = 1.10074, acc = 0.33333\n",
      "Epoch: 7, att_val: [0.9072201  0.09278046]\n",
      "Training: loss = 0.13532, acc = 0.97333 | Val: loss = 1.10109, acc = 0.33333\n",
      "Epoch: 8, att_val: [0.89710265 0.10289758]\n",
      "Training: loss = 0.11564, acc = 0.97500 | Val: loss = 1.10146, acc = 0.33333\n",
      "Epoch: 9, att_val: [0.9011481  0.09885197]\n",
      "Training: loss = 0.09397, acc = 0.98167 | Val: loss = 1.10185, acc = 0.33333\n",
      "Epoch: 10, att_val: [0.89069813 0.10930308]\n",
      "Training: loss = 0.08388, acc = 0.98167 | Val: loss = 1.10224, acc = 0.33333\n",
      "Epoch: 11, att_val: [0.8957663  0.10423309]\n",
      "Training: loss = 0.07961, acc = 0.97833 | Val: loss = 1.10259, acc = 0.33333\n",
      "Epoch: 12, att_val: [0.8395095  0.16049099]\n",
      "Training: loss = 0.08105, acc = 0.97667 | Val: loss = 1.10274, acc = 0.33333\n",
      "Epoch: 13, att_val: [0.8951166  0.10488418]\n",
      "Training: loss = 0.06640, acc = 0.98000 | Val: loss = 1.10282, acc = 0.33333\n",
      "Epoch: 14, att_val: [0.9046585  0.09534081]\n",
      "Training: loss = 0.05771, acc = 0.98167 | Val: loss = 1.10287, acc = 0.33333\n",
      "Epoch: 15, att_val: [0.91026545 0.08973402]\n",
      "Training: loss = 0.05482, acc = 0.99000 | Val: loss = 1.10292, acc = 0.33333\n",
      "Epoch: 16, att_val: [0.8870456  0.11295467]\n",
      "Training: loss = 0.05592, acc = 0.98667 | Val: loss = 1.10294, acc = 0.33333\n",
      "Epoch: 17, att_val: [0.89074636 0.109253  ]\n",
      "Training: loss = 0.04552, acc = 0.98833 | Val: loss = 1.10295, acc = 0.33333\n",
      "Epoch: 18, att_val: [0.8850927  0.11490817]\n",
      "Training: loss = 0.04040, acc = 0.99167 | Val: loss = 1.10297, acc = 0.33333\n",
      "Epoch: 19, att_val: [0.89376    0.10623941]\n",
      "Training: loss = 0.03698, acc = 0.98833 | Val: loss = 1.10302, acc = 0.33333\n",
      "Epoch: 20, att_val: [0.886303   0.11369719]\n",
      "Training: loss = 0.03440, acc = 0.98667 | Val: loss = 1.10311, acc = 0.33333\n",
      "Epoch: 21, att_val: [0.87925935 0.12073913]\n",
      "Training: loss = 0.02619, acc = 0.99167 | Val: loss = 1.10320, acc = 0.33333\n",
      "Epoch: 22, att_val: [0.8587989  0.14120063]\n",
      "Training: loss = 0.04205, acc = 0.98500 | Val: loss = 1.10310, acc = 0.33333\n",
      "Epoch: 23, att_val: [0.8911747  0.10882606]\n",
      "Training: loss = 0.02884, acc = 0.98667 | Val: loss = 1.10303, acc = 0.33333\n",
      "Epoch: 24, att_val: [0.91933656 0.080663  ]\n",
      "Training: loss = 0.02607, acc = 0.99333 | Val: loss = 1.10294, acc = 0.33333\n",
      "Epoch: 25, att_val: [0.9334053  0.06659439]\n",
      "Training: loss = 0.04762, acc = 0.98333 | Val: loss = 1.10288, acc = 0.33333\n",
      "Epoch: 26, att_val: [0.943898  0.0561012]\n",
      "Training: loss = 0.02765, acc = 0.99167 | Val: loss = 1.10296, acc = 0.33333\n",
      "Epoch: 27, att_val: [0.9434157  0.05658425]\n",
      "Training: loss = 0.02465, acc = 0.99333 | Val: loss = 1.10311, acc = 0.33333\n",
      "Epoch: 28, att_val: [0.9346971  0.06530333]\n",
      "Training: loss = 0.02243, acc = 0.99333 | Val: loss = 1.10322, acc = 0.33333\n",
      "Epoch: 29, att_val: [0.9366457  0.06335414]\n",
      "Training: loss = 0.01832, acc = 0.99500 | Val: loss = 1.10327, acc = 0.33333\n",
      "Epoch: 30, att_val: [0.94079965 0.059199  ]\n",
      "Training: loss = 0.02713, acc = 0.99500 | Val: loss = 1.10319, acc = 0.33333\n",
      "Epoch: 31, att_val: [0.93399316 0.06600455]\n",
      "Training: loss = 0.02640, acc = 0.98500 | Val: loss = 1.10304, acc = 0.33333\n",
      "Epoch: 32, att_val: [0.9382945  0.06170576]\n",
      "Training: loss = 0.01756, acc = 0.99500 | Val: loss = 1.10295, acc = 0.33333\n",
      "Epoch: 33, att_val: [0.94000894 0.0599916 ]\n",
      "Training: loss = 0.01661, acc = 0.99500 | Val: loss = 1.10288, acc = 0.33333\n",
      "Epoch: 34, att_val: [0.93467236 0.06532823]\n",
      "Training: loss = 0.01703, acc = 0.99167 | Val: loss = 1.10291, acc = 0.33333\n",
      "Epoch: 35, att_val: [0.93129885 0.06870059]\n",
      "Training: loss = 0.02366, acc = 0.99000 | Val: loss = 1.10301, acc = 0.33333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m fd\u001b[38;5;241m.\u001b[39mupdate(fd2)\n\u001b[0;32m    105\u001b[0m fd\u001b[38;5;241m.\u001b[39mupdate(fd3)\n\u001b[1;32m--> 106\u001b[0m loss_value_vl, acc_vl \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m val_loss_avg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_value_vl\n\u001b[0;32m    109\u001b[0m val_acc_avg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc_vl\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('build graph...')\n",
    "with tf.Graph().as_default():  # 创建一个新的计算图\n",
    "    with tf.name_scope('input'):  # 创建一个上下文管理器\n",
    "        ftr_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,  # 占位符，提前分配必要的内存\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),  # batch_size:1, nb_nodes:3025, fea_size: 1870\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]  # fea_list,长度为3，内部单个元素，(1, 3025, 1870)\n",
    "        bias_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]  # 邻接矩阵转换成的biases_list: 2. 单个元素占位符tensor, (1, 3025, 3025)\n",
    "        lbl_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes, nb_classes),   # tensor, nb_classes: 3\n",
    "                                        name='lbl_in')   \n",
    "        msk_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes),  # tensor, (1, 3025)\n",
    "                                        name='msk_in')\n",
    "        attn_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='attn_drop')  # tensor, ()\n",
    "        ffd_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')  # # tensor, ()\n",
    "        is_train = tf.compat.v1.placeholder(dtype=tf.bool, shape=(), name='is_train')  # # tensor, ()\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor(1, 3025, 3025)\n",
    "                                                       hid_units=hid_units,   # hid_units:[8]\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])  # （3025， 3）\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])  # （3025， 3）\n",
    "    msk_resh = tf.reshape(msk_in, [-1])              # mask，（3025， ）\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)  # 占位符计算softmax cross_entropy based on (pred, y)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)           # 计算accuracy\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)  # lr = 0.005、l2_coef = 0.001\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver()  # 用于保存模型\n",
    "\n",
    "    init_op = tf.group(tf.compat.v1.global_variables_initializer(),  # 全局变量初始化；group组合多个operation\n",
    "                       tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:  # 创建session\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):  # 200\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                # feature,占位符内存已经分配完毕，fea_list是真实数据，输入进行训练模型\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict:3. 每个元素tensor, (1, 3025, 1870)\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                # bias\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict: 2. 每个元素tensor, (1, 3025, 3025)\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                # other params\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0.6,\n",
    "                       ffd_drop: 0.6}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)  # 字典update方法\n",
    "                fd.update(fd3)  # 获得字典形式的所有数据、参数\n",
    "                # training操作：更新权重；计算loss；计算accuracy；attention概率\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "            # =============   val       =================\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                # fd1 = {ftr_in: features[vl_step * batch_size:(vl_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_val[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       msk_in: val_mask[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "          \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                                                 feed_dict=fd)\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step,\n",
    "                   val_loss_avg / vl_step, val_acc_avg / vl_step))\n",
    "            \n",
    "            # =============   judging  =================\n",
    "            if val_acc_avg / vl_step >= vacc_mx or val_loss_avg / vl_step <= vlss_mn:\n",
    "                if val_acc_avg / vl_step >= vacc_mx and val_loss_avg / vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg / vl_step\n",
    "                    vlss_early_model = val_loss_avg / vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg / vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg / vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn,\n",
    "                          ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ',\n",
    "                          vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        \n",
    "        # loading model params\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "        \n",
    "        # ============= testing =================\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "\n",
    "        print('start knn, kmean.....')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "  \n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        # xx = xx / LA.norm(xx, axis=1)\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "\n",
    "        my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f046e4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### debuge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac1313",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcc166",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "503.965px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
