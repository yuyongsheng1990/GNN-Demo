{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022.09.13，复现tri-party deep network representation\\ngithub address: https://github.com/GRAND-Lab/TriDNR\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.09.13，复现tri-party deep network representation\n",
    "github address: https://github.com/GRAND-Lab/TriDNR\n",
    "packages:\n",
    "    gensim==3.8.3\n",
    "    scikit-learn==1.1.2\n",
    "    numpy==1.23.3\n",
    "    pandas==1.4.4\n",
    "    scipy==1.9.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Tri-party Deep Network Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntri-party DNR is based on python package gensim and DeepWalk.\\ndataset: \\n    docs.txt: title information of each node in the network, each line represents a node(paper). The first item in each line is the node ID.\\n    adjedges.txt: neighbor nodes of each node in a network. the first node ID is the pivot node, and the rest items are the neighbor nodes linking to the first node.\\n    labels.txt: class labels of a node. Each line represents a node ID and its class label.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tri-party DNR is based on python package gensim and DeepWalk.\n",
    "dataset: \n",
    "    docs.txt: title information of each node in the network, each line represents a node(paper). The first item in each line is the node ID.\n",
    "    adjedges.txt: neighbor nodes of each node in a network. the first node ID is the pivot node, and the rest items are the neighbor nodes linking to the first node.\n",
    "    labels.txt: class labels of a node. Each line represents a node ID and its class label.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networkutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from random import shuffle\n",
    "from deepwalk import graph\n",
    "import gensim\n",
    "import random\n",
    "import gensim.utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkSentence = namedtuple('NetworkSentence', 'words tags labels index')\n",
    "Result = namedtuple('Result', 'alg trainsize acc macro_f1 micro_f1')\n",
    "AlgResult = namedtuple('AlgResult', 'alg trainsize numfeature mean std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNetworkData(dir, stemmer=0):  # dir, directory of network dataset\n",
    "    allindex = {}\n",
    "    alldocs = []\n",
    "    labelset = set()\n",
    "    with open(dir+'/docs.txt', 'r', encoding='utf-8') as f1, open(dir+'/labels.txt', 'r', encoding='utf-8') as f2:\n",
    "        for l1 in f1:\n",
    "#             tokens = ut.to_unicode(l1.lower()).split()\n",
    "            if stemmer == 1:\n",
    "                l1 = gensim.parsing.stem_text(l1)  # step_text=lower() + step()\n",
    "            else:\n",
    "                l1 = l1.lower()\n",
    "            tokens = ut.to_unicode(l1).split()  # to_unicode()转换成unicode编码\n",
    "            \n",
    "            words = tokens[1:]  # extract texts of document\n",
    "            tags = [tokens[0]]  # ID of each document, for doc2vec model\n",
    "            index = len(alldocs)\n",
    "            allindex[tokens[0]] = index  # A mapping from documentID to index, start from 0\n",
    "            \n",
    "            l2 = f2.readline()\n",
    "            tokens2 = gensim.utils.to_unicode(l2).split()\n",
    "            labels = tokens2[1]  # class label\n",
    "            labelset.add(labels)\n",
    "            alldocs.append(NetworkSentence(words, tags, labels, index))\n",
    "    return alldocs, allindex, list(labelset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDoc2Vec(doc_list=None, buildvoc=1, passes=20, dm=0, size=100, dm_mean=0, window=5,\\\n",
    "                hs=1, negative=5, min_count=1, workers=4):\n",
    "    model = Doc2Vec(dm=dm, size=size, dm_mean=dm_mean, window=window, hs=hs, negative=negative,\\\n",
    "                   min_count=min_count, workers=workers) # PV-DBOW\n",
    "    if buildvoc == 1:\n",
    "        print('Building Vocabulary')\n",
    "        model.build_vocab(doc_list)  # build vocabulate with words + nodeID\n",
    "    \n",
    "    for  epoch in range(passes):\n",
    "        print('Iteration %d ....' % epoch)\n",
    "        shuffle(doc_list)  # shuffling gets best results\n",
    "        model.train(doc_list, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWord2Vec(doc_list=None, buildvoc=1, passes=20, sg=1, size=100, dm_mean=0, window=5, hs=1,negative=5,\n",
    "                 min_count=1, workers=4):\n",
    "    model = Word2Vec(size=size, sg=sg, window=window, hs=hs, negative=negative, min_count=min_count, workers=workers)\n",
    "    if buildvoc == 1:\n",
    "        print('Building Vocabulary')\n",
    "        model.build_vocab(doc_list)  # build vocabulate with words + nodeID\n",
    "    for epoch in range(passes):\n",
    "        print('Iteration %d ...' % epoch)\n",
    "        shuffle(doc_list)\n",
    "        model.train(doc_list, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdeepwalks(directory, number_walks=50, walk_length=10, seed=1):\n",
    "    Graph = graph.load_adjacencylist(directory+'/adjedges.txt')\n",
    "    print('Number of nodes: {}'.format(len(Graph.nodes())))\n",
    "    num_walks = len(Graph.nodes()) * number_walks\n",
    "    print('Number of walks: {}'.format(num_walks))\n",
    "    \n",
    "    print('Walking...')\n",
    "    walks = graph.build_deepwalk_corpus(Graph, num_paths=number_walks, path_length=walk_length, alpha=0,\n",
    "                                        rand=random.Random(seed))\n",
    "    networksentence = []\n",
    "    raw_walks = []\n",
    "    for i, x in enumerate(walks):\n",
    "        sentence = [gensim.utils.to_unicode(str(t)) for t in x]\n",
    "        s = NetworkSentence(sentence, [sentence[0]], None, i) # label information is not used by random walk\n",
    "        networksentence.append(s)\n",
    "        raw_walks.append(sentence)\n",
    "    return raw_walks, networksentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coraEdgeFileToAdjfile(edgfile, adjfile, nodec):\n",
    "    edgeajd = {str(n): list() for n in range(ndoc)}\n",
    "    with open(edgefile, 'r') as f:\n",
    "        for l in f:\n",
    "            tokens = l.split()\n",
    "            degeadj[token[0]].append(tokens[1])\n",
    "            \n",
    "    wf = open(adjfile, 'w')\n",
    "    for n in range(nodoc):\n",
    "        edgestr = ' '.join(map(str, edgeadj[str(n)]))\n",
    "        wf.write(str(n) + ' ' + edgestr + '\\n')\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cora10groupdataset():\n",
    "    groupindex = {}\n",
    "    groupmap = {}\n",
    "    with open('data2/Cora/CoraHierarchyTree.txt', 'r') as f:\n",
    "        for l in f:\n",
    "            tokens = l.split('\\t')\n",
    "            if len(tokens) <= 2:\n",
    "                continue\n",
    "            elif len(tokens) == 3:\n",
    "                currentindex = len(groupindex)\n",
    "                groupindex[tokens[1]] = currentindex\n",
    "            elif len(tokens) == 5:\n",
    "                groupmap[tokens[3]] = currentindex\n",
    "            elif len(tokens) == 6:\n",
    "                groupmap[tokens[4]] = currentindex\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(train_vec, test_vec, train_y, test_y, classifierStr='SVM', normalize=0):\n",
    "    if classifierStr == 'KNN':\n",
    "        print('Training NN classifier...')\n",
    "        classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "    else:\n",
    "        print('Training SVM classifier...')\n",
    "        classifier = LinearSVC()\n",
    "    if normalize == 1:\n",
    "        print('Normalize data')\n",
    "        allvec = list(train_vec)\n",
    "        allvec.extend(test_vec)\n",
    "        allvec_normalized = preprocessing.normalize(allvec, norm='l2', axis=1)\n",
    "        train_vec = allvec_normalized[0:len(train_y)]\n",
    "        test_vec = allvec_normalized[len(train_y):]\n",
    "    \n",
    "    # training\n",
    "    classifier.fit(train_vec, train_y)\n",
    "    y_pred = classifier.predict(test_vec)\n",
    "    cm = confusion_matrix(test_y, y_pred)  # 混淆矩阵\n",
    "    print(cm)\n",
    "    acc = accuracy_score(test_y, y_pred)\n",
    "    print(acc)\n",
    "    macro_f1 = f1_score(test_y, y_pred, pos_label=None, average='macro')\n",
    "    micro_f1 = f1_score(test_y, y_pred, pos_label=None, average='micro')\n",
    "    \n",
    "    percent = len(train_y) * 1.0/(len(train_y) + len(test_y))\n",
    "    print('Classification method:' + classifierStr + '(train, test, Training_percent): (%d, %d, %f)' %\n",
    "          (len(train_y), len(test_y), percent))\n",
    "    print('Classification Accuracy=%f, macro_f1=%f, micro_f1=%f' % (acc, macro_f1, micro_f1))\n",
    "    # print(metrics.classification_report(test_y, y_pred))\n",
    "    return acc, macro_f1, micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluationEmbedModelFromTrainTest(model, train, test, classifierStr='SVM', normalize=0):\n",
    "    if isinstance(model, Doc2Vec):\n",
    "        # model.docvecs函数将文档ID转换为doc2vec向量\n",
    "        train_vecs = [model.docvecs[doc.tags[0]] for doc in train]  \n",
    "        test_vecs = [model.docvecs[doc.tags[0]] for doc in test]\n",
    "    else: # word2vec model\n",
    "        train_vecs = [model.wv.get_vector(doc.tags[0]) for doc in train]\n",
    "        test_vecs = [model.wv.get_vector(doc.tags[0]) for doc in test]\n",
    "    train_y = [doc.labels for doc in train]\n",
    "    test_y = [doc.labels for doc in test]\n",
    "    print('train_y: , test_y: ', len(train_y), len(test_y))\n",
    "    acc, macro_f1, micro_f1 = evaluation(train_vecs, test_vecs, train_y, test_y, classifierStr, normalize)\n",
    "    \n",
    "    return acc, macro_f1, micro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-party DNR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriDNR:\n",
    "    '''\n",
    "    Tri-party Deep Network Representation, IJCAI-2016\n",
    "    Read data from a from a directory which contains text, label, structure information, and initialize the TriDNR from\n",
    "    Doc2Vec and DeepWalk Models, then iteratively update the model with text, label, and structure information.\n",
    "    'directory'\n",
    "        docs.txt -- text document for each node, one line for one node\n",
    "        labels.txt -- class label for each node, noe line for one node\n",
    "        adjedges.txt -- edge list for each node, one line for one node\n",
    "    'train_size': percentage of training data in range 0-1, if train_size==0, it becomes pure unsupervised network representation\n",
    "    'text_weight': weights for the text information, 0-1\n",
    "    'size': the dimensionality of the feature vectors.\n",
    "    'dm': defines doc2vec the training algorithm. dm=1, PV_DM; otherwise, PV-DBOW.\n",
    "    'min_count': minimum number of counts for words.\n",
    "    '''\n",
    "    def __init__(self, directory=None, train_size=0.3, textweight=0.8, size=300, seed=1, workers=1, passes=10, dm=0,\n",
    "                min_count=3):\n",
    "        # Read the data\n",
    "        alldocs, docindex, classlabels = readNetworkData(directory)\n",
    "        print('%d document, %d classes, training ratio=%f' % (len(alldocs), len(classlabels), train_size))\n",
    "        \n",
    "        # Initialize Doc2Vec\n",
    "        if train_size > 0:  # label information is available for learning\n",
    "            print('Adding Label Information')\n",
    "            train, test = train_test_split(alldocs, train_size=train_size, random_state=seed)\n",
    "            '''\n",
    "            add supervised information to training data, use label information for learning.\n",
    "            Specifically, the doc2vec algorithm used the tags information as document IDs, and learn a vector\n",
    "                representation for each tag(ID),.\n",
    "            We add the class label into tags, so each class will acts as a ID and is used to learn the latent representation.\n",
    "            '''\n",
    "            alldata = train[:]\n",
    "            for x in alldata:\n",
    "                x.tags.append('Label'+x.labels)\n",
    "            alldata.extend(test)\n",
    "        else:  # no label information is availabel, pure unsupervised learning\n",
    "            alldata = alldocs[:]\n",
    "        \n",
    "        d2v = trainDoc2Vec(alldata, workers=workers, size=size, dm=dm, passes=passes, min_count=min_count)\n",
    "        \n",
    "        raw_walks, netwalks = getdeepwalks(directory, number_walks=20, walk_length=8)\n",
    "        w2v = trainWord2Vec(raw_walks, buildvoc=1, passes=passes, size=size, workers=workers)\n",
    "        if train_size > 0:  # print out the initial results\n",
    "            print('initialize Doc2Vec Model with supervised Information...')\n",
    "            evaluationEmbedModelFromTrainTest(d2v, train, test, classifierStr='SVM')\n",
    "            print('Initialize DeepWalk model')\n",
    "            evaluationEmbedModelFromTrainTest(w2v, train, test, classifierStr='SVM')\n",
    "        \n",
    "        self.d2v = d2v\n",
    "        self.w2v = w2v\n",
    "        self.doctags = [doc.tags[0] for doc in alldocs]\n",
    "        \n",
    "        self.train(d2v, w2v, directory, alldata, passes=passes, weight=textweight)\n",
    "        \n",
    "        if textweight > 0.5:\n",
    "            self.model = d2v\n",
    "        else:\n",
    "            self.model = w2v\n",
    "        \n",
    "    def setWeights(self, d2v_model, w2v_model, weight=1):\n",
    "        if isinstance(d2v_model, Doc2Vec):\n",
    "            print('Copy weights from Doc2Vec to Word2Vec')\n",
    "            keys = w2v_model.wv.vocab.keys()\n",
    "            for key in keys:\n",
    "                if key not in self.doctags:\n",
    "                    continue\n",
    "                w2v_index = w2v_model.wv.vocab[key].index  # word2Vec index\n",
    "                w2v_model.wv.syn0[w2v_index] = (1-weight) * w2v_model.wv.syn0[w2v_index] + \\\n",
    "                                weight * d2v_model.docvecs[key]\n",
    "\n",
    "    def train(self, d2v, w2v, directory, alldata, passes=10, weight=0.9):\n",
    "        raw_walks, walks = getdeepwalks(directory, number_walks=20, walk_length=10)\n",
    "        for i in range(passes):\n",
    "            print('Iterative Runing %d' % i)\n",
    "            self.setWeights(d2v, w2v, weight=weight)\n",
    "            # Train Word2Vec\n",
    "            shuffle(raw_walks)\n",
    "            print('Update W2V...')\n",
    "            w2v.train(raw_walks, total_examples=w2v.corpus_count, epochs=w2v.epochs)\n",
    "            self.setWeights(w2v, d2v, weight=(1-weight))\n",
    "\n",
    "            print('Update D2V...')\n",
    "            shuffle(alldata)  # shuffling to get best results\n",
    "            d2v.train(alldata, total_examples=d2v.corpus_count, epochs=d2v.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA Demo comparing several network representation algorithms:\\n\\nDoc2Vec: paragraph vector model which only use text information.\\nDeepWalk: DeepWalk algorithm which only use structure information.\\nDoc2Vec + DeepWalk: simple combination of Doc2Vec and DeepWalk model.\\n\\nTri-party DNR: tri-party DNR model, published in IJCAI-2016.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A Demo comparing several network representation algorithms:\n",
    "\n",
    "Doc2Vec: paragraph vector model which only use text information.\n",
    "DeepWalk: DeepWalk algorithm which only use structure information.\n",
    "Doc2Vec + DeepWalk: simple combination of Doc2Vec and DeepWalk model.\n",
    "\n",
    "Tri-party DNR: tri-party DNR model, published in IJCAI-2016.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10310 document\n",
      "10 classes\n"
     ]
    }
   ],
   "source": [
    "numFea = 100\n",
    "cores = 4\n",
    "train_size = 0.2  # percentage of training samples\n",
    "random_state = 2\n",
    "dm = 0\n",
    "passes = 20\n",
    "\n",
    "directory = 'tri-party data/M10'\n",
    "alldocs, allsentence, classlabels = readNetworkData(directory)\n",
    "print('%d document' % len(alldocs))\n",
    "print('%d classes' % len(classlabels))\n",
    "doc_list = alldocs[:]  # for reshuffling pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(doc_list, train_size=train_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "baseline 1, Doc2Vec Model dm=0\n",
      "Building Vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 ....\n",
      "Iteration 1 ....\n",
      "Iteration 2 ....\n",
      "Iteration 3 ....\n",
      "Iteration 4 ....\n",
      "Iteration 5 ....\n",
      "Iteration 6 ....\n",
      "Iteration 7 ....\n",
      "Iteration 8 ....\n",
      "Iteration 9 ....\n",
      "Iteration 10 ....\n",
      "Iteration 11 ....\n",
      "Iteration 12 ....\n",
      "Iteration 13 ....\n",
      "Iteration 14 ....\n",
      "Iteration 15 ....\n",
      "Iteration 16 ....\n",
      "Iteration 17 ....\n",
      "Iteration 18 ....\n",
      "Iteration 19 ....\n",
      "Classification Performance on Doc2Vec Model\n",
      "train_y: , test_y:  2062 8248\n",
      "Training SVM classifier...\n",
      "[[316   4  22  19  58  31  54  93  31  22]\n",
      " [ 18  26  10   9  31   7  28  33   4  13]\n",
      " [ 15   6 730  63  21  30  61  38  33  23]\n",
      " [ 12   7  68 707  20  60  37  32  27  49]\n",
      " [ 28   7  28  23 713  18  48  38  20  26]\n",
      " [ 16   5  21  78  21 547  39  31  30  43]\n",
      " [ 52  10  40  37  25  46 493 144  69  26]\n",
      " [ 72   9  21   9  62  37 147 521  22  26]\n",
      " [ 17   7  36  71  28  31 106  40 422  31]\n",
      " [ 41   8  37  90  93  90  57  45  42 440]]\n",
      "0.5959020368574199\n",
      "Classification method:SVM(train, test, Training_percent): (2062, 8248, 0.200000)\n",
      "Classification Accuracy=0.595902, macro_f1=0.555685, micro_f1=0.595902\n",
      "#############\n"
     ]
    }
   ],
   "source": [
    "# baselin 1, Doc2Vec model(PV-DM)\n",
    "print('#############')\n",
    "print('baseline 1, Doc2Vec Model dm=%d' % dm)\n",
    "doc2vec_model = trainDoc2Vec(doc_list, workers=cores, size=numFea, dm=dm, passes=passes, min_count=3)\n",
    "\n",
    "print('Classification Performance on Doc2Vec Model')\n",
    "doc2vec_acc, doc2vec_macro_f1, doc2vec_micro_f1 = \\\n",
    "    evaluationEmbedModelFromTrainTest(doc2vec_model, train, test, classifierStr='SVM')\n",
    "print('#############')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "baseline 2, DeepWalk model\n",
      "Number of nodes: 10310\n",
      "Number of walks: 206200\n",
      "Walking...\n",
      "Building Vocabulary\n",
      "Iteration 0 ...\n",
      "Iteration 1 ...\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "Iteration 6 ...\n",
      "Iteration 7 ...\n",
      "Iteration 8 ...\n",
      "Iteration 9 ...\n",
      "Iteration 10 ...\n",
      "Iteration 11 ...\n",
      "Iteration 12 ...\n",
      "Iteration 13 ...\n",
      "Iteration 14 ...\n",
      "Iteration 15 ...\n",
      "Iteration 16 ...\n",
      "Iteration 17 ...\n",
      "Iteration 18 ...\n",
      "Iteration 19 ...\n"
     ]
    }
   ],
   "source": [
    "# baseline 2, DeepWalk model\n",
    "print('#############')\n",
    "print('baseline 2, DeepWalk model')\n",
    "raw_walks, netwalks = getdeepwalks(directory, number_walks=20, walk_length=8)\n",
    "deepwalk_model = trainWord2Vec(raw_walks, buildvoc=1, sg=1, passes=passes, size=numFea, workers=cores)\n",
    "print('classification performance on DeepWalk model')\n",
    "doc2vec_acc, doc2vec_macro_f1, doc2vec_micro_f1 = \\\n",
    "    evaluationEmbedModelFromTrainTest(deepwalk_model, train, test, classifierStr='SVM')\n",
    "print('##############')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_12200\\2635918879.py:4: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  d2v_train_vecs = [doc2vec_model.docvecs[doc.tags[0]] for doc in train]\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_12200\\2635918879.py:5: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  d2v_test_vecs = [doc2vec_model.docvecs[doc.tags[0]] for doc in test]\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_12200\\2635918879.py:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  dw_train_vecs = [deepwalk_model.wv.word_vec(doc.tags[0]) for doc in train]\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_12200\\2635918879.py:8: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  dw_test_vecs = [deepwalk_model.wv.word_vec(doc.tags[0]) for doc in test]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "baseline 3, simple combination of DeepWalk + Doc2Vec\n",
      "train_y: , test_y:  2062 8248\n",
      "Classifcation Performance on Doc2Vec + DeepWalk\n",
      "Training SVM classifier...\n",
      "[[359  15   8   7  13  16  85 129  12   6]\n",
      " [ 26  31   5   8   2   0  56  36   7   8]\n",
      " [  5  13 764  48  22  32  15  14  74  33]\n",
      " [ 20   5  54 687  23  57  38  27  55  53]\n",
      " [ 12   3  23   7 769  25  14  16  30  50]\n",
      " [ 11   4  33  70  39 537  20  21  44  52]\n",
      " [ 70  24  23  16  12  30 531 176  43  17]\n",
      " [ 95  26  11  10  22  22 185 520  19  16]\n",
      " [  8  10  46  41  50  41  48  31 469  45]\n",
      " [ 28  11  41  84  83 101  26  33  63 473]]\n",
      "0.6231813773035888\n",
      "Classification method:SVM(train, test, Training_percent): (2062, 8248, 0.200000)\n",
      "Classification Accuracy=0.623181, macro_f1=0.582696, micro_f1=0.623181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# baseline 3, D2V+DW\n",
    "print('##############')\n",
    "print('baseline 3, simple combination of DeepWalk + Doc2Vec')\n",
    "d2v_train_vecs = [doc2vec_model.docvecs[doc.tags[0]] for doc in train]\n",
    "d2v_test_vecs = [doc2vec_model.docvecs[doc.tags[0]] for doc in test]\n",
    "\n",
    "dw_train_vecs = [deepwalk_model.wv.word_vec(doc.tags[0]) for doc in train]\n",
    "dw_test_vecs = [deepwalk_model.wv.word_vec(doc.tags[0]) for doc in test]\n",
    "\n",
    "train_y = [doc.labels for doc in train]\n",
    "test_y = [doc.labels for doc in test]\n",
    "\n",
    "# concanate two vectors\n",
    "train_vecs = [np.append(l, dw_train_vecs[i]) for i,l in enumerate(d2v_train_vecs)]\n",
    "test_vecs = [np.append(l, dw_test_vecs[i]) for i,l in enumerate(d2v_test_vecs)]\n",
    "\n",
    "print('train_y: , test_y: ', len(train_y), len(test_y))\n",
    "print('Classifcation Performance on Doc2Vec + DeepWalk')\n",
    "\n",
    "acc, macro_f1, micro_f1 = evaluation(train_vecs, test_vecs, train_y, test_y, classifierStr='SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tri-dnr method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10310 document, 10 classes, training ratio=0.200000\n",
      "Adding Label Information\n",
      "Building Vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 ....\n",
      "Iteration 1 ....\n",
      "Iteration 2 ....\n",
      "Iteration 3 ....\n",
      "Iteration 4 ....\n",
      "Iteration 5 ....\n",
      "Iteration 6 ....\n",
      "Iteration 7 ....\n",
      "Iteration 8 ....\n",
      "Iteration 9 ....\n",
      "Number of nodes: 10310\n",
      "Number of walks: 206200\n",
      "Walking...\n",
      "Building Vocabulary\n",
      "Iteration 0 ...\n",
      "Iteration 1 ...\n",
      "Iteration 2 ...\n",
      "Iteration 3 ...\n",
      "Iteration 4 ...\n",
      "Iteration 5 ...\n",
      "Iteration 6 ...\n",
      "Iteration 7 ...\n",
      "Iteration 8 ...\n",
      "Iteration 9 ...\n",
      "initialize Doc2Vec Model with supervised Information...\n",
      "train_y: , test_y:  2062 8248\n",
      "Training SVM classifier...\n",
      "[[387  15  23  11  19  15  63  72  22  23]\n",
      " [ 27  46  13  11  10   9  24  19   7  13]\n",
      " [ 13  28 710  99  10  11  46  38  45  20]\n",
      " [ 15  11  73 692  18  59  48  30  21  52]\n",
      " [ 33   3  23  22 698  15  45  41  30  39]\n",
      " [ 27   2  28  74  18 500  30  60  41  51]\n",
      " [ 30  18  27  42  17  46 573 113  54  22]\n",
      " [ 62   8  18  20  33  36 128 570  32  19]\n",
      " [ 16  11  36  45  28  33  91  28 471  30]\n",
      " [ 38  18  28  70  65  69  54  43  22 536]]\n",
      "0.6283947623666344\n",
      "Classification method:SVM(train, test, Training_percent): (2062, 8248, 0.200000)\n",
      "Classification Accuracy=0.628395, macro_f1=0.597546, micro_f1=0.628395\n",
      "Initialize DeepWalk model\n",
      "train_y: , test_y:  2062 8248\n",
      "Training SVM classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   2  11   9   8  20   2 564  19  13]\n",
      " [  0   0   6   7   1  11   2 143   1   8]\n",
      " [  0   7 691  75  48 104   6   1  37  51]\n",
      " [  1   2  76 656  50 110   4   3  42  75]\n",
      " [  0   3  61  66 656  63   6   1  45  48]\n",
      " [  0   5  54 119  53 418  11   5  46 120]\n",
      " [  1   6  34  35  22  54   7 726  30  27]\n",
      " [  0   6  22  19  24  28   2 782  21  22]\n",
      " [  0   8  65  81  87 166   4   3 283  92]\n",
      " [  1   7  79 172  92 197   8   4  61 322]]\n",
      "0.4627788554801164\n",
      "Classification method:SVM(train, test, Training_percent): (2062, 8248, 0.200000)\n",
      "Classification Accuracy=0.462779, macro_f1=0.361170, micro_f1=0.462779\n",
      "Number of nodes: 10310\n",
      "Number of walks: 206200\n",
      "Walking...\n",
      "Iterative Runing 0\n",
      "Copy weights from Doc2Vec to Word2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_12420\\4064101611.py:68: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  w2v_model.wv.syn0[w2v_index] = (1-weight) * w2v_model.wv.syn0[w2v_index] + \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 1\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 2\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 3\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 4\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 5\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 6\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 7\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 8\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n",
      "Iterative Runing 9\n",
      "Copy weights from Doc2Vec to Word2Vec\n",
      "Update W2V...\n",
      "Update D2V...\n"
     ]
    }
   ],
   "source": [
    "tridnr_model = TriDNR(directory, size=numFea, dm=0, textweight=0.8, train_size=train_size, seed=random_state,\n",
    "                     passes=10)\n",
    "evaluationEmbedModelFromTrainTest(tridnr_model.model, train, test, classifierStr='SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
